---
title: 'How to consume and publish Kafka'
sidebar_label: 'Consume and publish Kafka'
id: ht-consume-kafka
keywords: [kafka, integration, ingest]
tags:
    - kafka
    - integration
    - ingest
---



import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Kafka is a standard messaging infrastructure used throughout financial markets.  Genesis has the ability to consume messages from Kafka topics, and to publish messages to Kafka topics.  This is handled transactionally and can be done in a few different ways, depending on the desired functionality.

## Consuming from Kafka
This is done using a Genesis pipeline.  A pipeline allows Genesis to process incoming, and outgoing, messages and can be used for a variety of connection methods including csv files as well as Kafka topics.

To implement a pipeline the following is needed in the processes.xml file:
```jsx
<processes>
  <process name="MYAPP_MANAGER">
    ...
    <module>genesis-pal-datapipeline</module>
    <package>global.genesis.pipeline</package>
    <script>myapp-data-pipelines.kts</script>
    ...
  </process>
</processes>
```

Kafka is then configured in the pipeline kts file like this:

```jsx
val source = kafkaSource<String, Int> {
    bootstrapServers = systemDefinition.getItem("BOOTSTRAP_SERVER").toString()
    groupId = systemDefinition.getItem("CONSUMER_GROUP_ID").toString()
    keyDeserializer = StringDeserializer()
    valueDeserializer = IntegerDeserializer()
    topic = systemDefinition.getItem("KAFKA_TOPIC").toString()
    securityProtocol = systemDefinition.getItem("KAFKA_SECURITY_CONFIG").toString()
}

val operator: SplitOperator<ConsumerRecords<String, Int>, ConsumerRecord<String, Int>> = SplitOperator { consumerRecords ->
    flow {
        consumerRecords.forEach {
            emit(it)
        }
    }
}

pipelines {
    pipeline("KAFKA_TO_DB_PIPELINE") {
        source(source)
            .split(operator)
            .map {
                PriceReceived(it.key(), it.value())
            }
            .map {
                DbOperation.Insert(it)
            }
            .sink(txDbSink())
    }
}
```

The resulting message can be processed and stored in the database and can also trigger events via:
```jsx
    codey codey codey
```

There is an example application here __need link__ that shows this in action.

## Publishing to Kafka
This can be done in several different ways:
- Triggered by an event.  You can use the AbstractProgrammaticSource to create a source implementation in one line that allows you to send to the pipeline from an event
```jsx
package global.genesis.kafka

import global.genesis.gen.dao.PricePublished
import global.genesis.pipeline.event.AbstractProgrammaticSource

object ProgrammaticPriceSource : AbstractProgrammaticSource<PricePublished>()
```
Then in the event handler a call of:
```jsx
  eventHandler<PricePublished>(name = "PRICE_PUBLISH") {
      onCommit { event ->
        val price = event.details
        entityDb.insert(price)
        ProgrammaticPriceSource.send(price)
        ack()
      }
  }
```


- Use a database subscription source which detects changes and publishes these to kafka
__code?__

- Using scheduled cron rules to trigger the execution of a pipeline and publish the related data to kafka
__code?__

There is an example application here __need link__ that shows these in action.

## Data types
All standard data/message types can be supported by Genesis, including strings, json, XML, FIX format
More detials on these can be found here __need link__.


## Testing

:::info
Go to our [**Testing**](/how-to/ht-consume-kafka-test/) page for details of how to test Kafka in your application.
:::

