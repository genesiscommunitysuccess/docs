---
title: 'Data Pipelines - Implementations'
sidebar_label: 'Implementations'
id: implementations
keywords: [server, pipelines, implementations]
tags:
  - server
  - pipelines
  - implementations
---

## Sources

### Camel File Source (Realtime)

Realtime acknowledgement source that uses camel to periodically poll a location and returns a flow of type PipelineFile which includes the file name, file path and input stream of its contents.
The next element will only be emitted once acknowledgement is given that the previous one has been successfully processed.
The location must be provided upon instantiation of this source, as shown below, and any camel specific configurations should be included within this string.

GPAL Example:
```kotlin
pipeline("TEST_CAMEL_ROUTE_PIPELINE") {
	source(camelSource {
      location = "file://src/test/resources/camel-source-test?noop=true&idempotent=true"
  }).sink(filesSink)
}
```

### Database Batch Poll Source (Realtime)

Realtime acknowledgement source that returns a stream of updates via a batch poll on a particular database dependent on an acknowledgement having been received of the previous element. If no data is received, the source will wait for a delay before attempting to poll again. This source implementation uses the PersistenceManager to persist the latest index using the provided lambda - by default the DbPersistenceManager is used to manage the state of this source. See Persistence Manager for more details (link to persistence manager).

The type of object that you would like this source to return must be given as part of the source definition, example shown below.

When configuring the db batch poller to use as part of your pipeline you must provide the following as part of the configuration:
- source (required): this is what the PersistenceManager will use to find the last persisted record and should be unique across all pipelines
- index (required): this is the table index used to order the records and get the value for the last persisted record for recovery - if single field index is provided, ensure this value is of type Long. Else, see below.
- buildIndex (required for multi field index only): if you would like to use an index which contains multiple fields, you need to provide a lambda which converts from a Long value (the last persisted value) to your EntityIndex object using this value - you must ensure that the Long value which you would like to be used as the persistence index needs to be the final value inside your index object
- dbLookupDelayMs (optional): this is the delay time that the poller will wait for in milliseconds after not receiving any new data from the database - it is configured to 200ms by default and is not required as part of the provided config

GPAL Example:
```kotlin
// Single Field Index Example
val batchPollSource = dbBatchQuery<TestIndexTable> {
    source = "SINGLE_INDEX_SOURCE"
    index = TestIndexTable.ByIndex
}

// Multiple Field Index Example
val batchPollMultiIndexSource = dbBatchQuery<TestMultiIndexTable> {
    source = "MULTI_INDEX_SOURCE"
    index = TestMultiIndexTable.ByNameIndex
    buildIndex { value -> TestMultiIndexTable.ByNameIndex("NAME", value) }
}

pipelines {
    pipeline("TEST_BATCH_POLL_PIPELINE") {
        source(batchPollSource)
            .sink(testSink)
    }
}
```

## Operators

### CSV Decoder

Split Operator to decode from an InputStream (from a PipelineFile object) to a CsvRow object which contains:

1. the line number for each CSV row
2. an object of a user defined type representing the data of each CSV line, by default this is of type `Map<String, String>`

GPAL functions available are `csvRawDecoder()` to deserialise to `Map<String, String>` and `csvDecoder<Trade>()` to deserialise to specified class.

GPAL Examples:
```kotlin
// Decoding to Map<String, String>
pipeline("TEST_PIPELINE_RAW_DECODER") {
    source(camelSource {
        location = ""
    }).split(csvRawDecoder()) // Default to deserialising to Map<String, String>
        .map { input: CsvRow<Map<String, String>> ->
            DbOperation.Insert(
                System {
                    this.systemKey = input.data["SYSTEM_KEY"]!!
                    this.systemValue = input.data["SYSTEM_VALUE"]!!
                }
            )
        }
        .sink(txDbSink())
}

// Decoding to another class
pipeline("TEST_PIPELINE_DECODER") {
    source(camelSource {
        location = ""
    }).split(csvDecoder<System>()) // Deserialise to System class
        .map { input: CsvRow<System> ->
            DbOperation.Insert(input.data)
        }
        .sink(txDbSink())
}

// User Defines SimpleTrade Class for type of each CSV Row
pipeline("TEST_CSV_DECODER") {
        source(CsvDecoderTestSource(File(getResourcePath("/Trades.csv"))))
            .split(CsvDecoder(SimpleTrade::class.java))
            .sink(CsvDecoderTestSink())
    }

```

## Sinks

### Database Sink

A sink into the Genesis database which takes a DbOperation object as input with an object extending TableEntity i.e.  `INPUT : DbOperation<out TableEntity>` .

DbOperation has 4 subtypes: Insert, Modify, Upsert and Delete.

GPAL functions available are `dbSink()` for non-transactional and `txDbSink()` for transactional.

GPAL Examples:

```kotlin
// Transactional db sink
pipeline("TEST_DB_SINK") {
    source(dbBulkSubscribe<Trade>())
        .map {
            val record = when (it) {
                Bulk.Prime.Completed -> null
                is Bulk.Prime.Record -> it.record
                is Bulk.Update.Delete -> null
                is Bulk.Update.Insert -> it.record
                is Bulk.Update.Modify -> it.record
            }
            DbOperation.Insert(record!!)
        }
        .sink(txDbSink())
}

// Non-transactional db sink
pipeline("TEST_DB_SINK") {
  source(dbBulkSubscribe<Trade>())
      .map {
          val record = when (it) {
              Bulk.Prime.Completed -> null
              is Bulk.Prime.Record -> it.record
              is Bulk.Update.Delete -> null
              is Bulk.Update.Insert -> it.record
              is Bulk.Update.Modify -> it.record
          }
          DbOperation.Insert(record!!)
      }
      .sink(dbSink())
}
```

## Other

### On Completion Handler

Handler that executes after a successful pipeline. Multiple handlers can be added to a pipeline.

For Batch pipelines, this is at the end of the whole pipeline.

For Realtime pipelines, this is at the end of each processed element.

GPAL Example:

```kotlin
pipeline("TEST_ON_COMPLETION_PIPELINE") {
    source(dbBulkRealtimeSource)
        .map(mapper)
        .sink(logSink)
        .onCompletion { context: PipelineContext<Bulk<Trade>> ->
            // do something
        }
}
```

### Send Event On Completion Handler
 TODO

See [Document Management Pipelines](/components/doc-management/doc-management-server/doc-management-pipelines/) for more implementations.
