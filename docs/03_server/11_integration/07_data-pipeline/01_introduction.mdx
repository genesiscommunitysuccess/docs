---
title: 'Data Pipeline - introduction'
sidebar_label: 'Introduction'
id: introduction
keywords: [server, integration, data pipeline, introduction]
tags:
  - server
  - integration
  - data pipeline
  - introduction
---

Genesis Data Pipelines enable you to stream data into and out of your Genesis application. If you are looking to integrate your application with an external database or file system, this module should be your first consideration. If Data Pipeline does not offer you the solution you need, we suggest using [Apache Camel](/server/integration/apache-camel/introduction/) to create the integration. 

Each Data Pipeline defines a source, some mapping operations and a sink.

| term | description |
| ---- | ----------- |
| source | Changes in data at this location trigger your Data Pipeline |
| operators | Transform data from the data source |
| sink | Does something with the mapped data from the data source |

### Source
Your pipeline needs a data source. The example below points to a local filesystem location. It pulls in any file that is dropped into the local file system’s directory **run/runtime/coppClarkSimple**:

```kotlin
source(
    camelSource { 
        location = "file://run/runtime/coppClarkSimple"
    }
)
```

### Operators
Operators are used to transform input format to output format (although they can do more). You can chain them together, where each operator simply takes the output type of the previous one. 

So in this case, the output of `.split` goes into `.map` and the output type of `.map` is a list of DbOperations, which the next sink (dbSync) can take as input.

```kotlin
            //1
            .split(csvRawDecoder())
            //2
            .map { row ->
                DbOperation.Upsert( ExchangeHoliday {
                    //The exclamations at the end tell the code "we expect this to always be present in the row data"
                    holidayDate = parseCCDateFormat(row.data["EventDate"])!!
                    holidayName = row.data["EventName"]
                    holidayType = getHolidayType(row.data["FileType"])!!
                    isoMic = row.data["ISO MIC Code"]!!
                })
            }
```

The example above has two operators. Let's look at them more closely.

- `split` uses the pre-built decoder which converts the CSV file and outputs a map containing each row in column + value: `CsvRow<Map<String, String>>`
- `map` takes the data we need from the row columns and formats it accordingly. We are building up ExchangeHoliday, a database entity in our app, wrapped in the Db operation we want to perform on it; in this case, we want to upsert.

For more complex requirements, you can create your own [transformer](#keeping-the-data-transformation-separate) in a seperate file. You can see this in two of our examples.

Also, you can build multiple database entities from a single file. You can see details of this in the [third example](#updating-multiple-different-entities-from-a-single-source).

### Sink
Once the data is in the required format, we want to sink it. For file ingress, this means updating the database. 

The example sink below takes the output from `.map` (a set of DB operations) and handles the updating of the database.

```kotlin
.sink(dbSink())
```

That’s all you need to ingest the data and put it in your database.

However, there are additional helpers for things such as `onCompletion`. This enables you specify what happens after the file processing is complete. 

In the example below, we use this to log that we’ve completed it, and to send a notification to users:

```kotlin
.onCompletion { context ->
    LOG.info("Completed processing of file ${context.data.name}")
}.onCompletion(
    notifyAllScreensOnCompletion {
        body = "Finished processing filename ${context.data.name}"
        header = "Finished Processing Copp Clark Exchange Holidays File"
        severity = Severity.Information
    }
)
```

[comment]: <> (Waiting on error handling to add a section on it here)

## Data Pipeline ingress

You can define Data Pipelines that map data from an external source (database, file) to [tables](/database/fields-tables-views/tables/) in your application. By default, the resulting Table objects are stored in the application database. If you want to change this behaviour, you can define [custom sink operations](/server/integration/data-pipeline/advanced/#auditable-sink-operations).

Each Data Pipeline defines a source for the data and maps that data to each [field](/database/fields-tables-views/fields/) in the table.

If a field mapping is not one-to-one - e.g. complex type conversions, data obfuscation, enriched values - you can define a `transform` function that returns `Any`.

Here is a sample configuration:
```kotlin
pipelines {

    postgresSource("cdc-test") {
        hostname = "localhost"
        port = 5432
        username = "postgres"
        password = "docker"
        databaseName = "postgres"

        table {
            "public.source_trades" to map("incoming_trades", TRADE) {
                val tradeId = stringValue("trd_id")
                val tradedAt = longValue("traded_at")

                TRADE {

                    TRADE_TYPE {
                        property = "side"
                    }

                    TRADE_DATE {
                        transform {
                            DateTime(input.get(tradedAt))
                        }
                    }

                    RECORD_ID {
                        transform {
                            input.get(tradeId).removePrefix("ITS_").toLong()
                        }
                    }
                }
            }
        }
    }
}
```

Once your Genesis application is running, data ingestion will take place.

## Data Pipeline egress

Data Pipelines can also be defined to listen to changes within your application's database and react to these changes. These changes can be mapped and then sinked into an external database.

Here is a sample configuration:
```kotlin
val postgresConfig = postgresConfiguration(
    databaseName = "test",
    hostname = "localhost",
    port = 5432,
    username = "test",
    password = "test"
)

pipelines {
    genesisTableSource(TRADE) {
        key = TRADE.BY_ID

        map("", TRADE) {
            TRADE {
                TRADE.TRADE_ID {
                    property = "id"
                }
                TRADE.INSTRUMENT_ID {
                    property = "instrument"
                }
                TRADE.PRICE {
                    property = "price"
                    transform {
                        "$ $input"
                    }
                }
                TRADE.QUANTITY {
                    property = "quantity"
                }
            }
        }.sink(postgresConfig) {
            onInsert = insertInto("outTable")
            onDelete = deleteFrom("outTable")
            onModify = updateTable("outTable")
        }
    }
}
```

## Supported sources

The currently supported sources are:

**Ingress:**
- PostgreSQL
- MS SQL Server
- Oracle Enterprise
- Files
  - CSV
  - XML
  - JSON

**Egress:**
- Genesis application database

## Supported sinks

Ingress:
- Genesis database (default)
- Custom sinks

Egress:
- All SQL based databases over JDBC are supported.