---
title: 'Data Pipelines - Implementations'
sidebar_label: 'Implementations'
id: implementations
keywords: [server, pipelines, implementations]
tags:
  - server
  - pipelines
  - implementations
---

## Sources

### Camel File Source (Realtime)

This is a realtime acknowledgement source that uses camel to poll a location periodically. It returns a flow of type `PipelineFile`, which includes the file name, file path and input stream of its contents.

The next element is only emitted once acknowledgement is given that the previous one has been successfully processed.

The location must be provided upon instantiation of this source, as shown below. Any camel-specific configurations must be included within this string.

GPAL Example:
```kotlin
pipeline("TEST_CAMEL_ROUTE_PIPELINE") {
	source(camelSource {
      location = "file://src/test/resources/camel-source-test?noop=true&idempotent=true"
  }).sink(filesSink)
}
```

### Database Batch Poll Source (Realtime)

This is a realtime acknowledgement source that returns a stream of updates via a batch poll on a particular database, dependent on an acknowledgement having been received of the previous element.

If no data is received, the source waits for a delay before attempting to poll again. This source implementation uses the PersistenceManager to persist the latest index using the provided lambda; by default the DbPersistenceManager is used to manage the state of this source. See Persistence Manager for more details (link to persistence manager).

The type of object that you would like this source to return must be given as part of the source definition, as shown in the example below.

When configuring the db batch poller to use as part of your pipeline you must provide the following as part of the configuration:

- source (required): this is what the PersistenceManager will use to find the last persisted record and should be unique across all pipelines
- index (required): this is the table index used to order the records and get the value for the last persisted record for recovery - if single field index is provided, ensure this value is either the generated `TIMESTAMP` field in the table or of type Long. Else, see below.
- buildIndex (required for multi-field index only): if you would like to use an index that contains multiple fields, you need to provide a lambda that converts from a Long value (the last persisted value) to your EntityIndex object using this value - you must ensure that the Long value which you would like to be used as the persistence index needs to be the final value inside your index object
- dbLookupDelayMs (optional): this is the delay time that the poller will wait for in milliseconds after not receiving any new data from the database - it is configured to 200ms by default and is not required as part of the provided config

Here is a GPAL example of a Batch Poll source definition:
```kotlin
// Single Field Index Example
val batchPollSource = dbBatchQuery<TestIndexTable> {
    source = "SINGLE_INDEX_SOURCE"
    index = TestIndexTable.ByIndex
}

// Multiple Field Index Example
val batchPollMultiIndexSource = dbBatchQuery<TestMultiIndexTable> {
    source = "MULTI_INDEX_SOURCE"
    index = TestMultiIndexTable.ByNameIndex
    buildIndex { value -> TestMultiIndexTable.ByNameIndex("NAME", value) }
}

pipelines {
    pipeline("TEST_BATCH_POLL_PIPELINE") {
        source(batchPollSource)
            .sink(testSink)
    }
}
```

### Abstract Programmatic Source

This is an abstract implementation of a realtime acknowledgement source that additionally has a send method which can be used by any custom code to provide data to the pipeline.

This source is dependent on the calling code being a part of the same process as the pipeline using this source - ensure this is done by updating your process definition. See [processes](../../../server/configuring-runtime/processes)

Here is an example of using this source to provide Trade objects:
```kotlin
object ProgrammaticTradeSource : AbstractProgrammaticSource<Trade>()
```

Here is an eventhandler that sends to this `ProgrammaticTradeSource`:

```kotlin
eventHandler<Trade>("TRADE_INSERT", transactional = true) {
    onCommit { event ->
      val trade = event.details
      ProgrammaticTradeSource.send(trade)
      ack()
    }
  }
```

The resulting pipeline:
```kotlin
pipelines {
    pipeline("TEST_BATCH_POLL_PIPELINE") {
        source(ProgrammaticTradeSource)
        .sink(testSink)
    }
}
```

## Operators

### CSV Decoder

This is a split Operator that decodes from an InputStream (from a PipelineFile object) to a CsvRow object that contains:

- the line number for each CSV row
- an object of a user-defined type representing the data of each CSV line; by default, this is of type `Map<String, String>`

GPAL functions available are:

- `csvRawDecoder()` to deserialise to `Map<String, String>`
- `csvDecoder<T>()` to deserialise to the specified class, represented by T here.

Here are some GPAL examples of a CSV Decoder:

```kotlin
// Decoding to Map<String, String>
pipeline("TEST_PIPELINE_RAW_DECODER") {
    source(camelSource {
        location = "file://src/test/resources/camel-source-test?noop=true&idempotent=true"
    }).split(csvRawDecoder()) // Default to deserialising to Map<String, String>
        .map { input: CsvRow<Map<String, String>> ->
            DbOperation.Insert(
                System {
                    this.systemKey = input.data["SYSTEM_KEY"]!!
                    this.systemValue = input.data["SYSTEM_VALUE"]!!
                }
            )
        }
        .sink(txDbSink())
}

// Decoding to another class
pipeline("TEST_PIPELINE_DECODER") {
    source(camelSource {
        location = "file://src/test/resources/camel-source-test?noop=true&idempotent=true"
    }).split(csvDecoder<System>()) // Deserialise to System class
        .map { input: CsvRow<System> ->
            DbOperation.Insert(input.data)
        }
        .sink(txDbSink())
}

```

## Sinks

### Database sink

This is a sink into the Genesis database, which takes a DbOperation object as input with an object extending TableEntity i.e.  `INPUT : DbOperation<out TableEntity>` .

DbOperation has 4 subtypes: Insert, Modify, Upsert and Delete.

GPAL functions available are:

- `dbSink()` for non-transactional
- `txDbSink()` for transactional

The GPAL example below contains both a transactional sink and a non-transactional sink:

```kotlin
// Transactional db sink
pipeline("TEST_DB_SINK") {
    source(dbBulkSubscribe<Trade>())
        .map {
            when (it) {
                    Bulk.Prime.Completed -> null
                    is Bulk.Prime.Record -> null
                    is Bulk.Update.Delete -> DbOperation.Delete(it.record)
                    is Bulk.Update.Insert -> DbOperation.Insert(it.record)
                    is Bulk.Update.Modify -> DbOperation.Modify(it.record)
                }
        }
        .sink(txDbSink())
}

// Non-transactional db sink
pipeline("TEST_DB_SINK") {
  source(dbBulkSubscribe<Trade>())
      .map {
          when (it) {
                    Bulk.Prime.Completed -> null
                    is Bulk.Prime.Record -> null
                    is Bulk.Update.Delete -> DbOperation.Delete(it.record)
                    is Bulk.Update.Insert -> DbOperation.Insert(it.record)
                    is Bulk.Update.Modify -> DbOperation.Modify(it.record)
                }
      }
      .sink(dbSink())
}
```

## Handlers

### On Completion Handler

This is a handler that executes after a successful pipeline operation. Multiple handlers can be added to a pipeline.

- For Batch pipelines, this is at the end of the whole pipeline.

- For Realtime pipelines, this is at the end of each processed element.

GPAL Example:

```kotlin
pipeline("TEST_ON_COMPLETION_PIPELINE") {
    source(dbBulkRealtimeSource)
        .map(mapper)
        .sink(logSink)
        .onCompletion { context: PipelineContext<Bulk<Trade>> ->
            // do something
        }
}
```

### Send Event On Completion Handler

This is a type of on completion handler that triggers an event.
It takes in the name of the event as well as the input object that the event will need to run.

For example, for an event that does some logic based on the Trade object:

```kotlin
eventHandler<TradeDetails>("TRADE_LOGIC", transactional = true) {
    onCommit { event ->
      val trade = event.details
      // further logic here using trade object
      ack()
    }
  }
```

The following handler can be used:

```kotlin

val insertTradeOnCompletionHandler = sendEventOnCompletion<Bulk<Bond>, TradeDetails> {
            eventName = "EVENT_TRADE_LOGIC"
            buildEvent { context: PipelineContext<Bulk<Bond>> ->
                TradeDetails(
                    // construct TradeDetails object based on provided context object
                )
            }
        }

pipeline("TEST_TRADE_PIPELINE") {
    source(dbBulkRealtimeSource)
        .map(mapper)
        .sink(logSink)
        .onCompletion(insertTradeOnCompletionHandler)
}
```

See [Document Management Pipelines](/components/doc-management/doc-management-server/doc-management-pipelines/) for more implementations.
