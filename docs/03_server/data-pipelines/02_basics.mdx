---
title: 'Data Pipelines - basics'
sidebar_label: 'Basics'
id: basics
keywords: [server, pipelines, basics]
tags:
  - server
  - pipelines
  - basics
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Set-up

You can configure your data pipelines in a file called _app-name_**-pipelines.kts**. This must be located in your application’s **scripts** directory. 

Once created, you need to add this script to your application’s [processes configuration](/server/data-pipelines/configuring-runtime/).

If you are **not** using the [Genesis Gradle Settings Plugin](/operations/project-structure/settings-plugin/) you need to add the following dependency to your app module:

:::note Only if you are not using the Gradle Settings Plugin
If you are using the [Genesis Gradle Settings Plugin](/operations/project-structure/settings-plugin/), you don't need to do this.
:::

<Tabs defaultValue="gradle" values={[{ label: 'Gradle', value: 'gradle', }, { label: 'Maven', value: 'maven', }]}>
    <TabItem value="gradle">

        ```kotlin
        implementation(genesis("pal-datapipeline"))
        ```

    </TabItem>
    <TabItem value="maven">

        ```xml
        <dependency>
            <groupId>global.genesis</groupId>
            <artifactId>genesis-pal-datapipeline</artifactId>
            <version>$genesisVersion</version>
        </dependency>
        ```

    </TabItem>
</Tabs>

## Pipelines

There are two kinds of pipeline:

- Batch pipeline
- Realtime pipeline

Each pipeline must have a single source and single sink. In between these stages, you can define any number of operator steps, as long as the types and ordering comply to that of the pipeline you are using.

See below for more details on what types are available for each type of pipelines.

### Batch pipeline

A batch pipeline will only run once and must be triggered manually via the use of its execute method. The pipeline can be retrieved from the Pipeline Manager class.

The execute method takes in a map of input parameters, which provide the context to be used throughout the pipeline.

Here is an example:

```kotlin
// GPAL Script to define pipeline
val fileStorageSource = fileStorageSource()

pipelines {
    pipeline("TEST_FILE_STORAGE") {
        source(fileStorageSource)
            .transform(StreamOperator<PipelineFile, String> { input -> flow {
                input.collect { pipelineFile ->
                    pipelineFile.content.bufferedReader().use { it.readText() } }
            }
            })
            .sink(fileSink)
    }
}
```

#### Triggering executionYou can trigger the execution of the above axample pipeline from an `eventHandler`.

```kotlin
val pipelineManager = inject<PipelineManager>()

eventHandler<FileStorageData>(name = "FILE_PIPELINE_START") {
        onCommit { event ->
            val details = event.details

            val pipeline = pipelineManager.getBatchPipeline("TEST_FILE_STORAGE")
            pipeline?.execute(mapOf("FILE_STORAGE_ID" to details.fileStorageId, "FILE_NAME" to details.fileName))
            ack()
        }
    }
```

### Realtime pipeline

A realtime pipeline is started as soon as the process running the script is healthy. It is only stopped once the process has shut down. 

If you need to stop or start the pipeline manually, you can do this using the Pipeline Manager; however, this is not recommended. 

There are two possible scenarios for a realtime pipeline:

**Streaming**

:::note
You must start off with a split operator, after which you can use any combination of operators.
:::

GPAL Example:

**Element**

:::note
You must start off with a suspend element operator, after which you can use any combination of operators.
:::

GPAL Example:



