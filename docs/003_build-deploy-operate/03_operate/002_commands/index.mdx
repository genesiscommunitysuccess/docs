---
title: 'Commands'
sidebar_label: 'Commands'
id: bdo-operate-commands
keywords: [operate, operations, commands]
tags:
- operate
- operations
- commands
sidebar_position: 2
---


Genesis has numerous built-in commands that have their own individual functions.
This page goes through them and details the function, parameters and use cases of those commands.

Once an application has been built and zipped, you can install it in any another system that contains the Genesis Application Platform.

To ensure a correct installation, you must follow the product installation procedure.

## Logging level for commands

The logging level for Genesis commands is set to "WARN" by default. 

To change the level for a command, set the environment variable `GENESIS_LOGGING_LEVEL` to any valid level ("ERROR", "WARN", "INFO", "DEBUG", "TRACE").

If you set `GENESIS_LOGGING_LEVEL` to a non-valid level, it is automatically reset to the default.

## `AppGen`

<!-- The AppGen command has a list of possible arguments: -d, -t, -p, -pn  -->

AppGen can be used to generate a fully working application from a dictionary file.

Usually when creating a application, you would start with a schema; you then build Data Servers, Request Servers and Event Handlers on top to create your application. AppGen automates all this, and will generate the following new files:

- Data Server: _application_**-dataserver.kts**
- Request Server: _application_**-reqrep.kts**
- Event Handler: _application_**-eventhandler.kts**
- System processes: _application_**-processes.xml**
- Service definitions: _application_**-service-definitions.xml

These processes can also (or instead) load definitions from compiled Kotlin classes in scanned packages; see [GPAL Standards](/develop/server-capabilities/gpal-standards/#script-and-compiled-definitions). Command usage is unchanged.

###### Data Server and Request Server

One block will be generated for each table in the database, complete with metadata (all fields on the table) and a field block that returns all fields on the table. For Request Servers, the inbound metadata will be based on the primary key.

###### Event Handler

The file for the Event Handler contains insert, amend and delete transactions for every table in the database.  All transactions support validation and meta data. If a field is marked as a sequence in the dictionary (i.e. generated ID) then the field is not specified on the metadata for inserts, but it will be specified on modifies and deletes.

Deletes will have reduced metadata, as it is only necessary for the columns to satisfy the primary key to perform the delete.

###### Parameters

| Argument | Argument long name | Mandatory | Description | Restricted Values |
| -- | -- | -- | -- | -- |
| -d <!-- AppGen -d --> | dictionary file name | true | the name of the dictionary to read at startup |No |
| -t <!-- AppGen -t --> | table name(s) | false | the table name(s) within the dictionary to read at startup. Multiple tables must be separated by spaces |No |
| -p <!-- AppGen -p --> | port offset | true | the port range to use when generating the services file |No |
| -pn <!-- AppGen -pn --> | product name | true | the name of the product to create |No |

###### Examples

This example has no `-t` option.

```bash
AppGen -d tas-dictionary.xml -p 4000 -pn tas
```

In this case, the dictionary to read is `tas-dictionary.xml`, the port offset is `4000` and the product name to generate is `tas`. Running this command results in the following structure being created:

```bash
tas/
    /bin
    /cfg/tas-dataserver2.xml
        /tas-fields-dictionary.kts
        /tas-processes.xml
        /tas-reqrep.xml
        /tas-service-definitions.xml
        /tas-system-definition.kts
        /tas-tables-dictionary.kts
    /scripts/tas-tnHandler.gy
```

This example has a `-t` option, which specifies that only the table's `ORDER` and `USER` should have code generated for them.

```bash
AppGen -d tas-dictionary.xml -t ORDER USER -p 4000 -pn tas
```

AppGen components for `ORDER` and `USER` tables will be appended to the relevant files that were created in the `tas` directory.

## `CountRecords`

This counts the number of records in the database, grouped by table; the results are printed to screen.

By default, a count is given for every table in the database.

If you only want the record count for specific tables, you can specify the table name or names.

<!-- The CountRecords command has one argument where you can send a space-separated list of tables -->

###### Syntax
| Argument | Argument long name | Mandatory | Description                                            | Restricted values | Default |
|----------|--------------------|-----------|--------------------------------------------------------|-------------------|-------|
|          |          [TABLES] <!-- CountRecords [TABLES] -->  | no        | the name of the table whose rows are to be counted; for more than one table, this must be a space-separated list            | none                | a record count is provided for all tables in the database    |

This example gives a record count for all tables:

```bash
CountRecords
```

The example below gives a record count for two specific tables:

```bash
CountRecords CHF_ORDERS CHF_TRADES
```

## `CreateMissingSqlSequences`

:::warning
Stop all your application's processes before using this command.
:::

This command creates sequences in the database for ID fields that rely on auto-generated sequence values.

The command checks to see which sequences exist already, and only creates new sequences where they do not exist. Consequently, if you run the command more than once, you will get the same result each time.

This command is applicable only for SQL databases and should only be executed when you are [enabling Sequences for SQL databases](/develop/server-capabilities/data-model/#sequence).

###### Syntax

The `CreateMissingSqlSequences` command has no parameters.

### Working with different databases
The behavior of this command depends on which database implementation your application uses.

- **If you are using a NOSQL database**, such as Foundation DB, auto-incremented values are assigned in blocks of 100 in order to improve performance. This command sets the value in the database, which corresponds to the first value in the next range to be allocated.

- **If you are using Oracle**, you can **not** set a sequence value directly. This command increments the sequence value by the difference between the current counter value and the desired value. This can have unexpected effects on sequence values that are already assigned in the cache, as the increment is also applied to these values.

And remember, only use this command when all your applications have been stopped. After running `SetAutoIncrement`, you need to restart the server.


## `DbMon`

DbMon is the Genesis database client, which provides its own command line. From here, you can navigate through the database tables in your application.

`DbMon` itself has a `help` command, which shows all the available commands. To get help on a specific command, run `help _command_`.

`DbMon --quietMode` performs database changes without triggering real-time updates in the update queue layer.

For full details, see our page on [DbMon](/build-deploy-operate/operate/commands/dbmon/).

## `DictionaryBuilder`

`DictionaryBuilder` connects to an RDBMS, parses schemas and uses this information to generate a Genesis tables dictionary. It supports SQLServer and PostgreSQL databases.

The script accepts a series of arguments to establish a connection to the database and to control the behaviour of the script:

| Arg | Long argument              | Mandatory | Description                                                                                                  |
|-----|----------------------------|-----------|--------------------------------------------------------------------------------------------------------------|
| -j  | --jdbcUrl \<jdbcUrl\>      | Yes       | JDBC URL of the database.                                                                                    |
| -p  | --product \<productName\>  | Yes       | represents the product name and affects the output files                                                   |
| -c  | --createFormat             |           | generates JSON ready for use with Genesis Create, instead of the standard tables dictionary                 |
| -e  | --export                   |           | exports table data to CSV                                                                                    |
<!-- | -g  | --genesisTables       |           | reads from existing Genesis database                                                                          | -->he
| -t  | --tables \<table1,table2>  |           | table list to include, default is all tables                                                                 |
| -o  | --output \<outputDir\>     |           | output directory for files; if the directory does not exist, it will be created. Defaults to project root.   |
| -s  | --schema \<schemaId\>      |           | Schema name / System ID                                                                                      |
|     | --tableid \<tableIdStart\> |           | Table ID start number, default = 2000; ignored if reading from Genesis DB.                                   |

###### Example

Generates a Genesis tables dictionary from a PostgreSQL database and exports data to a TABLE_NAME.csv file in the root of the project:
```shell
DictionaryBuilder --jdbcUrl jdbc:postgresql://localhost:5432/?user=postgres&password=docker --product my-app --export
```

The example below generates JSON for use with Genesis Create from an SqlServer database:
```shell
DictionaryBuilder --jdbcUrl jdbc:sqlserver://localhost:1433;trustServerCertificate=true;username=SA;password=VerySecure123 --product my-app --createFormat
```

The example below generates a Genesis tables dictionary from a PostgreSQL database called `myDatabase` 
```shell
DictionaryBuilder --jdbcUrl jdbc:postgresql://localhost:5432/myDatabase?user=postgres&password=docker --product my-app --export --tables user_trades
```

###### How the script behaves

The script connects to the specified JDBC Url and retrieves the information it requires to build a Genesis tables dictionary.

There are a few considerations you should be aware of:

* Table and column names parsed in lowerCamelCase format will be transformed to UPPER_SNAKE_CASE
* If there is no mapping available for the `java.sql.Type` the column will be ignored and the name logged in the output.
* Exporting extremely large datasets with the `--export` flag is not supported.

###### Keys and indexes
Primary keys will be parsed as primary keys in Genesis, whether they are single-column-based or multiple-column-based.

Only unique indexes will be parsed as secondary keys.

There is no concept of foreign keys in Genesis, so these are ignored.

## `DropTable`

The `DropTable` command removes all rows from the specified tables.

:::warning
If you do not specify any tables, then **all** rows will be removed from **all** tables in the database.

Use with care!
:::

<!-- The DropTable command has one possible argument: -t -->

###### Syntax 
| Argument | Argument long name | Mandatory | Description                                            | Restricted values | Default |
|----------|--------------------|-----------|--------------------------------------------------------|-------------------|-------|
| -t <!-- DropTable -t [TABLES] -->      | [TABLES]    | no        | the name of the table to have its rows removed; for more than one table, this must be a space-separated list | none | all records are removed from all tables |

The example below removes all the rows from three separate tables:

```bash
DropTable -t EQUITY_ORDERS_GB EQUITY_ORDERS_DE EQUITY_ORDERS_IT
```

The command will ask you to confirm the removal of the content from each table.

## `DumpIt`
To copy data from a Genesis database, use the `DumpIt` command.

###### Syntax
The `DumpIt` command can take the following arguments:
<!-- The DumpIt command has a list of possible arguments: -a, -f, -fields, -h, -s, -t, -cem, -fm, -qi, -where -->

| Argument | Argument long name | Mandatory | Description                                            | Restricted values | Default |
|----------|--------------------|-----------|--------------------------------------------------------|-------------------|-------|
| -a <!-- DumpIt -a -->      | --all <!-- DumpIt --all -->             | no        | exports all tables to csv                              | none              | none    |
| -f <!-- DumpIt -f -->      | --file (arg) <!-- DumpIt --file (args) -->    | no        | name of the csv file where table is exported           | none              | none    |
|          | -fields (arg) <!-- DumpIt --fields (args) -->   | no        | space separated field list e.g. "FIRST_NAME LAST_NAME" | none              | none    |
| -h <!-- DumpIt -h -->      | --help  <!-- DumpIt --help -->           | no        | show help on how to use the command                    | none              | none    |
| -s  <!-- DumpIt -s -->     | --sql (arg) <!-- DumpIt --sql (args) -->     | no        | name of the sql file to export the table to            | none              | none    |
| -t  <!-- DumpIt -t -->     | --table (arg) <!-- DumpIt --table (args) -->   | no        | specify the name of the table you want to export to CSV. If there are multiple tables to include, provide a list separated by spaces.                 | none              | none    |
| -cem <!-- DumpIt -cem -->    | --criteriaEvaluatorMode (arg) <!-- DumpIt --criteriaEvaluatorMode (args) --> | no        | the type of criteria evaluator to be used | TYPE_AWARE or LEGACY | LEGACY |
| -fm <!-- DumpIt -fm -->     | --formatMode (arg) <!-- DumpIt --formatMode (args) -->    | no        | indicates whether field formats should be taken into account  | FORMATTED (takes field formats into account) or LEGACY (does not take field formats into account) | none    |
| -qi <!-- DumpIt -qi -->     | --quoteIdentifiers <!-- DumpIt --quoteIdentifiers --> | no        | if present, all sql identifiers (e.g. column names) will be quoted     | none  | none   |
|          | -where (arg) <!-- DumpIt -where (args) -->    | no        | match criteria e.g. "USER_NAME=='John'"                | none              | none    |

Here are some examples:

```bash
DumpIt -t GBP_TRADES -f gbp-trades
```

This copies all records in the GBP_TRADES table to the file **gbp-trades.csv**.

```bash
DumpIt -t USER -where "USER_NAME=='John'" -fields "USER_NAME"
```

This copies the USER_NAME of every record in the USER table where the USER_NAME is John. This useful if you want to know if the user name John is in the database.


```bash
DumpIt -t FUND -f FUND -fields "FUND_ID NAME" -where "NAME == 'FUND_FUND' && YEARS_IN_SERVICE >= 10"
```

This copies the FUND_ID and NAME fields of every record that has "FUND_FUND" for a name, and ten or more years in service.


```bash
DumpIt --all
```

This copies all the tables in the system, creating one .csv file for each table in the database. The files are saved in the current directory. It is useful for taking a back-up of the current system database.

### Interactive mode
You can run `DumpIt` without any arguments to enter interactive mode.

## `FixEnumValues`

Converts non-matching enum values in the database to SNAKE_CASE. This command is intended for use after a dictionary change that adds new enum values. It will only update the data if the converted value matches the list of enum values in the dictionary.

Changes will only be applied to the database if the command is run with the **--commit** flag.

:::warning
Stop all processes before using this command.
:::

###### Syntax
The `FixEnumValues` command can take the following arguments:
<!-- The FixEnumValues command has a list of possible arguments: -c, -h -->

| Argument | Argument long name     | Mandatory | Description                                                                            | Restricted values | Default |
|----------|------------------------|-----------|----------------------------------------------------------------------------------------|----------------|--------|
| -c  <!-- FixEnumValues -c -->     | --commit  <!-- FixEnumValues --commit -->             | no        | applies dictionary changes to the database                                             | none    | none |
|          | [TABLES] <!-- FixEnumValues --commit [TABLES] -->              | no        | a space-separated list of specific tables to be changed; if no list is supplied, all tables are changed                                              | none              | none |
| -h <!-- FixEnumValues -h -->      | --help  <!-- FixEnumValues --help -->           | no        | shows help on how to use the command                    | none   | none    |

In the example below, the changes are applied to the database for two tables: TRADE and POSITION.

```bash
FixEnumValues --commit TRADE POSITION
```

## `genesisInstall`

This command validates all system and product configuration, checking for things such as field duplication, and ultimately installs config files into place to be picked up on the next system start: `/generated/cfg/`.

**System Definition token replacement**: For any files collected with file names with suffix `.tmplt.xml`, the command examines the installation environment and looks for system definition tokens. The generated `cfg` file names have their token placeholders replaced with the system definition value for the token assigned for that environment, and the suffix of the file will be changed to `.auto.xml`.

**Site Specific overridden files**: The command also checks the system-specific definitions and uses these to replace any definitions that have the same name in any of the modules. Where a file in `site-specific/cfg` has the same name as a file in a module's `cfg`, the version in `site-specific/cfg` will always be used.

`genesisInstall` also completes config checking, looking out for mistakes in the configured code and providing warnings and error messages. If an error is encountered, the configuration will not be propagated to the `generated/cfg` area and a message will be printed. See `--ignore` flag details below on how to ignore errors and copy the files regardless.

###### Syntax
The `genesisInstall` command can take the following arguments:

<!-- The genesisInstall command has a list of possible arguments: --ignore, --ignoreHooks, --compactProcesses, --repeatedHooks, --hostDiff -->


| Argument | Argument long name | Mandatory | Description                                                                                                               | Restricted values | Default |
|----------|--------------------|-----------|---------------------------------------------------------------------------------------------------------------------------------------|-------------------|--------|
|          | --ignore  <!-- genesisInstall --ignore -->         | no        | If supplied, will ignore errors in the configuration files                                                                            | none              | none |
|          | --ignoreHooks <!-- genesisInstall --ignoreHooks -->      | no        | If supplied, will ignore any install hooks found                                                                                      | none              | none |
|          | --compactProcesses <!-- genesisInstall --compactProcesses -->  | no        | When set to `true`, combines compatible services into fewer JVM processes (depending on the selected strategy); overall, this reduces the total number of services required to run | none                | none |
|          | --compactProcessesStrategy <!-- genesisInstall --compactProcessesStrategy -->  | no        | The strategy to be used when `compactProcesses` is set; defaults to `DEFAULT`; valid values are: `DEFAULT`, `DEV`. WARNING: the DEV strategy optimizes (and sometimes removes) processes to provide the optimal developer experience with the fewest possible resources; **DO NOT use this for production deployments** | none                | none |
|          | --repeatedHooks <!-- genesisInstall --repeatedHooks -->    | no        | If supplied, will repeat the specified install hooks                                                                                      | none              | none |
|          | --hostDiff  <!-- genesisInstall --hostDiff -->        | no        | If supplied, will compare all the files in every Genesis server in the cluster to make sure that the files are in sync  | none              | none |

The example below ignores errors and leaves the configuration files undeleted, even if errors are found.

```bash
genesisInstall --ignore
```

The example combines all compatible processes into a single process.

```bash
genesisInstall --compactProcesses
```

### Install hooks

Install hooks are scripts that run automatically the next time `genesisInstall` is run in an environment. They are typically operations that should run only once for a given environment, as part of an application version upgrade. If any install hook fails, an error will be provided and it will run again on the next `genesisInstall`.

You can specify an install hook script by adding it to your application codebase in the `server/{appName}-app/src/main/genesis/scripts/installHooks` directory.

The scripts must be implemented to work in an idempotent way, and the end result of executing any install hook script should mean the system is in the expected target state for the upgraded application to run. So it should work when running it on a pre-existing environment (e.g. upgrading an application environment) or if it is run in a completely new environment installing the application for the first time.

###### Status of install hook run

Any environment running an application needs to store the state of running any install hooks, so that the next run of `genesisInstall` knows whether to run them or not.

The system definition property `HookStateStore` defines how the environment determines if an install hook has already run:

- `HookStateStore="FILE"`: The install hook's log file output is used to determine whether each install hook has run successfully. Logs are stored in `GENESIS_HOME/runtime/installHooks`. The hook state is considered as successful if the last line of the latest log file reads "SUCCESS".

- `HookStateStore="DB"`: The status of each install hook is stored in the application's database in a table called `SYSTEM`. The hook key is stored in the `SYSTEM_KEY` column and the status is stored in the `SYSTEM_VALUE` column. This is read to determine if any given install hook script needs to run.

#### Running an install hook every time

On rare occasions, you might need an install hook that runs every time a `genesisInstall` is run - irrespective of its previous run status. For this requirement, use one of the following options:

- Add `_persist` to the end of the filename. With this, the script will run every time `genesisInstall` is run.
- Use the `genesisInstallHooks` command with the `repeatHooks` option, for example: `genesisInstallHooks --repeatHooks=auth,reporting`. See below for more details on this script.

#### `genesisInstallHooks`
The `genesisInstallHooks` command specifically runs a (set of) install hook scripts. This is needed when:

- you need to run install hooks without running `genesisInstall`
- you need to run `PRE_INSTALL` or `POST_REMAP` install hooks
    - `PRE_INSTALL` hooks are used when you need the script functionality to run **prior** to running `genesisInstall`. Scripts must be placed in the `/scripts/installHooks/preInstallHooks/` directory.
    - `POST_REMAP` hooks are used when you need the script functionality to run **after** a `remap` has been completed. Scripts must be placed in the `/scripts/installHooks/postRemapHooks/` directory.

| Argument           | Mandatory | Options      | Description                                                                                               |
|--------------------|-----------|--------------|-----------------------------------------------------------------------------------------------------------|
| --hookType         | false   | `POST_INSTALL` | Hook type can only take a single option. POST_INSTALL is the default where --hookType is omitted, which runs standard install hooks located in the `/scripts/installHooks/` directory  |
|                    |         | `PRE_INSTALL`  | `genesisInstallHooks --hookType=PRE_INSTALL` runs install hooks in the `/scripts/installHooks/preInstallHooks/` directory |
|                    |         | `POST_REMAP`   | `genesisInstallHooks --hookType=POST_REMAP` runs install hooks in the `/scripts/installHooks/postRemapHooks/` directory |
| --init             | false   |                | `genesisInstallHooks --init=true` marks all installHooks as completed without running any of them. Typically used on a brand new environment. If the HookStateStore is set to DB and this is a new environment, you must run remap before running with this flag. |
| --repeatHooks      | false   |                | `genesisInstallHooks --repeatHooks=my_app,auth,reporting` repeats ALL install hooks for a specified comma-separated list or modules, regardless of [their status](#status-of-install-hook-run)  |

Install hook file-name conventions:
- Genesis uses shell script for install hooks. Within the shell script, you could call a Python script, a Kotlin script, Java code, or whatever type of script is necessary.
- The install hook name must be unique within all the relevant directories for install hooks: `installHooks/`, `installHooks/preInstallHooks/` and `installHooks/postRemapHooks/`.
- Each install hook must contain a priority number + `_` at the beginning of the file name. This number must be unique. For example: **1_**migrateLogFiles.sh, **2_**migrateDictionary.sh.
- If you need to create a new install hook that has to execute before priority number 1 or number 2, you must increase the numbers for all the other scripts (e.g. rename **1_**migrateLogFiles.sh to **9_**migrateLogFiles.sh).

###### Practical examples

You can create a new script and add it to the `/scripts/installHooks/` directory to perform any particular functionality as part of `genesisInstall`.

Any script exiting with value "0" is considered successful by the installHooks system, and any script exiting with a non-zero value is considered to have failed execution.

In this example, we'll show 2 scripts. The first is the installHook script which calls the second, which is a kotlin script to update some database records:

```bash
#!/bin/bash
echo "InstallHook migrating EXCHANGE records with CODE null or empty, to be 'N/A'"

GenesisRun exchangeCodeNA-script.kts

exit $?
```

Then in `exchangeCodeNA-script.kts`:

```kotlin title=exchangeCodeNA-script.kts
import kotlinx.coroutines.runBlocking
import global.genesis.db.ModifyDetails


runBlocking {
    println("Starting script")
    val arrayList: ArrayList<ModifyDetails> = ArrayList()

    rxDb.getBulk("EXCHANGE").map { record ->
        if (record.getString("CODE") == null || record.getString("CODE") == "") {
            record.setString("CODE", "N/A")
            arrayList.add(ModifyDetails(record, "EXCHANGE_BY_ID", listOf("CODE")))
            println("Updated CODE for Exchange: " + record.getString("EXCHANGE_ID"))
        }
    }
    .distinct()
    .toList()
    .blockingGet()

    rxDb.modifyAll(arrayList).blockingSubscribe();
}
```

Consider another example; we have a migration script called `migrateDictionary.sh` as an install hook; this internally executes [MigrateDictionary](#migratedictionary) as shown below:

```shell
#!/bin/bash

MigrateDictionary -dst DB

exit $?
```

## `GenesisRun`

This is a Python script wrapper for Genesis scripts.

'GenesisRun` will attempt to find a script to execute within the Genesis directory structure (site-specific or scripts).

There are two environment variables that can be used to configure how much RAM the scripts will use:

* SCRIPT_MAX_HEAP
* REMAP_MAX_HEAP

`GenesisRun` can execute code in two different modes: Groovy script and GPAL Kotlin script. **GenesisRun** builds the necessary classpath, so you don't need to build it in each script.

* Groovy script: GenesisRun SetLogLevelScript.groovy
* GPAL Kotlin script: GenesisRun customPurger-script.kts

There is a separate wrapper, `JvmRun` for Java main class scripts.

## `GetAutoIncrementCount`
This gets the current auto increment INT values defined in dictionaries for all the sequences in the system. By default, the values are printed on screen (to the terminal), but they can be written to a file so they can be reused by the `SetAutoIncrement` script (see below).

:::warning
Stop all your application's processes before using this command.
:::

###### Syntax
The `GetAutoIncrementCount` command can take the following arguments:
<!-- The GetAutoIncrementCount command has a list of possible arguments: -f, -h, -p -->

| Argument | Argument long name | Mandatory |               Description               | Restricted values | Default |
|----------|--------------------|-----------|-----------------------------------------|-------------------|---------|
| -f  <!-- GetAutoIncrementCount -f -->     | --file (arg) <!-- GetAutoIncrementCount --file (args) -->    | no        | name of file to receive the values      | none              | AutoIncrementValues |
| -h <!-- GetAutoIncrementCount -h -->      | --help   <!-- GetAutoIncrementCount --help -->          | no        | displays help on the command            | none              | none    |
| -p  <!-- GetAutoIncrementCount -p -->     | --print  <!-- GetAutoIncrementCount --print -->          | no        | if sending to a file, then use this if you also want to print to screen | none             | true (unless -f is supplied) |

The behavior of this command depends on which database implementation your application uses.

- **If you are using a NOSQL database**, such as Foundation DB, auto-incremented values are assigned in blocks of 100 in order to improve performance. This command retrieves the value of the counter stored on disk. If the system is currently active, this value might not correspond to the value of the next record inserted that references the value.

- **Similarly, if you are using Oracle**, auto-incremented values are cached in memory in configurable block sizes. This command only retrieves the current value of the counter stored on disk.

- **If you are using an SQL implementation**, this command returns the last value assigned by the sequence, not the next to be assigned.

For example, this command puts all the auto increment values in a file called **AutoIncVals**. The values are not displayed on screen.

```bash
GetAutoIncrementCount -f=AutoIncVals
```
And remember: only use this command when all the application's processes have been stopped.

## `GetSequenceCount`

This gets the current sequence number for all the sequences in the system. The values can be printed on screen or written to a file so they can be reused by the `SetSequence` script (see below). For example, if you have 120 rows in the table DE_ORDERS, the sequence count is for that table is 120.

###### Syntax
The `GetSequenceCount` command can take the following arguments:
<!-- The GetSequenceCount command has a list of possible arguments: -f, -h, -p -->


| Argument | Argument long name | Mandatory |               Description               | Restricted values | Default |
|----------|--------------------|-----------|-----------------------------------------|-------------------|---------|
| -f  <!-- GetSequenceCount -f -->     | --file (arg) <!-- GetSequenceCount --file (args) -->     | no        | name of the file to contain the sequence numbers | none     | SEQUENCE.csv |
| -h <!-- GetSequenceCount -h -->       | --help <!-- GetSequenceCount --help -->             | no        | show help on how to use thus command             | none     | none |
| -p <!-- GetSequenceCount -p -->       | --print  <!-- GetSequenceCount --print -->           | no        |                                                  | none     | true (unless -f is supplied) |

The example below puts the numbers for all sequences in the database in the file **/home/user/run/sequenceCount**.
```
GetSequenceCount --file=/home/user/run/sequenceCount
```

## `GetNextSequenceNumbers`

This gives you the next sequence number of every table in the application. The numbers are provided in table format (csv), for example:

```
"Table","Sequence","Value"
"USER_AUDIT","UA","104"
"PROFILE_AUDIT","PR","804"
"PROFILE_USER_AUDIT","PA","104"
```
In the example above, the table USER_AUDIT has 103 rows, so the next sequence number for that table is 104.

###### Syntax
The `GetNextSequenceNumbers` command can take the following arguments:
<!-- The GetNextSequenceNumbers command has a list of possible arguments: -f, -h, -p -->


| Argument | Argument long name | Mandatory |               Description               | Restricted values | Default |
|----------|--------------------|-----------|-----------------------------------------|-------------------|---------|
| -f  <!-- GetNextSequenceNumbers -f -->     | --file (arg) <!-- GetNextSequenceNumbers --file (args) -->    | no        | name of the file to contain the sequence numbers | none     | SequenceValues.csv |
| -h  <!-- GetNextSequenceNumbers -h -->     | --help   <!-- GetNextSequenceNumbers --help -->          | no        | show help on how to use this command             | none     | none |
| -p  <!-- GetNextSequenceNumbers -p -->     | --print  <!-- GetNextSequenceNumbers --print -->          | no        |                                         | none              | true (unless -f is supplied) |

The example below displays the next sequence number of each table, using the format described above.
```bash
GetNextSequenceNumbers
```

By default, the details are sent to the screen, but you can redirect the output to a file. For this, use either the `-f` argument followed by the filename. Alternatively, you can use the `>>` annotation followed by a filename, as in the example below:

```bash
GetNextSequenceNumbers >> /tmp/NextSeqNumbers.txt
```

### Correcting errors in tables
The `GetNextSequenceNumbers` command is often used with the `SetSequence` script [see below](#setsequence), for example, if you suspect that you have an error in one of your tables:

1. Stop all the processes and run `GetNextSequenceNumbers` to find the next sequence numbers of the tables.
2. Check the table contents. You might find that a row is missing or needs to be added. Make this change on the database manually. This affects the sequence numbers in those tables.
3. Run `SetSequence` to reset the sequence numbers where relevant.
4. Now you can [restart your processes](#startserver).

## `GetNextAutoIncrementNumbers`

This gives you the next auto increment value for every table with auto increment fields in the application. The numbers are provided in table format (csv), for example:

```
"Table","Field","Value"
"USER_AUDIT","ID","104"
"PROFILE_AUDIT","PROFILE_ID","804"
"PROFILE_USER_AUDIT","AUDIT_ID","104"
```
In the example above, the table USER_AUDIT has 103 rows, so the next auto increment value for the ID field in that table is 104.

###### Syntax
The `GetNextAutoIncrementNumbers` command can take the following arguments:
<!-- The GetNextAutoIncrementNumbers command has a list of possible arguments: -f, -h, -p, --verbose -->


| Argument | Argument long name | Mandatory |               Description               | Restricted values | Default |
|----------|--------------------|-----------|-----------------------------------------|-------------------|---------|
| -f  <!-- GetNextAutoIncrementNumbers -f -->     | --file (arg) <!-- GetNextAutoIncrementNumbers --file (args) -->    | no        | name of the file to contain the auto increment values | none     | AUTO_INCREMENT.csv |
| -h  <!-- GetNextAutoIncrementNumbers -h -->     | --help   <!-- GetNextAutoIncrementNumbers --help -->          | no        | show help on how to use this command             | none     | none |
| -p  <!-- GetNextAutoIncrementNumbers -p -->     | --print  <!-- GetNextAutoIncrementNumbers --print -->          | no        | outputs the auto increment values to the console                                         | none              | true (unless -f is supplied) |
| --verbose  <!-- GetNextAutoIncrementNumbers --verbose -->     | --verbose  <!-- GetNextAutoIncrementNumbers --verbose -->          | no        | outputs the error stack trace if something goes wrong             | none     | false |

The example below displays the next auto increment value for each table with auto increment fields, using the format described above.
```bash
GetNextAutoIncrementNumbers
```

By default, the details are sent to the screen, but you can redirect the output to a file. For this, use either the `-f` argument followed by the filename. Alternatively, you can use the `>>` annotation followed by a filename, as in the example below:

```bash
GetNextAutoIncrementNumbers >> /tmp/NextAutoIncrementNumbers.txt
```

You can also use the `-p` flag to explicitly print to the console, or the `--verbose` flag to see detailed error information if something goes wrong.

### Correcting errors in tables
The `GetNextAutoIncrementNumbers` command is often used with the `SetAutoIncrement` script [see below](#setautoincrement), for example, if you suspect that you have an error in one of your tables:

1. Stop all the processes and run `GetNextAutoIncrementNumbers` to find the next auto increment values of the tables.
2. Check the table contents. You might find that a row is missing or needs to be added. Make this change on the database manually. This affects the auto increment values in those tables.
3. Run `SetAutoIncrement` to reset the auto increment values where relevant.
4. Now you can [restart your processes](#startserver).

## `killProcess`

This command is used to terminate a specified process.

###### Syntax
The `killProcess` command can take the following arguments:
<!-- The killProcess command has a list of possible arguments: -s, -f, -w, -c -->

| Argument                     | Argument long name                            | Mandatory | Description                                                                                                                                                                                      | Restricted values | Default |
|------------------------------|-----------------------------------------------|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-------|
| -s   HOSTNAME [HOSTNAME ...]  <!-- killProcess -s [HOSTNAME ...] --> | --hostname HOSTNAME HOSTNAME   [HOSTNAME ...] <!-- killProcess --hostname --> [HOSTNAME ...] | no        | Where the application is running on more than one node, this identifies the node where you want to kill the process (so you can kill a process on a different node). Specify the Host Name. | none              | none |
| -f   <!-- killProcess -f --> | --force   <!-- killProcess --force -->  |           | forcefully kills a process (using kill -9)       | none              | none |
| -w WAIT   <!-- killProcess -w --> | --wait WAIT     <!-- killProcess --wait -->        | no        | specifies how many seconds to wait before forcing the kill       | none         |  10 |
| -c     <!-- killProcess -c -->    | --cluster         <!-- killProcess --cluster -->  | no        | kills the process on every node in the cluster             | none              | none |

The example below kills the GENESIS_AUTH_PERMS process.

 ```bash
killProcess GENESIS_AUTH_PERMS
```
In the example below, there is a cluster; this command kills the GENESIS_ROUTER process on the nodes HARRY and GARY.

 ```bash
killProcess GENESIS_ROUTER --hostname HARRY, GARY
```

In the example below, there is a cluster; this command kills the GENESIS_ROUTER process on ALL nodes.

 ```bash
killProcess GENESIS_ROUTER --cluster
```

## `killServer`

This command kills the server. It reads the **$GC/processes.xml** file to determine which processes to kill. It will prompt for confirmation (`Are you sure you want to kill server? (y/n):`), unless you specify `--force`.

###### Syntax
The `killServer` command can take the following arguments:
<!-- The killServer command has a list of possible arguments: -s, -f, --all, -c -->

| Argument                     | Argument long name                            | Mandatory | Description                                                                                                                                                                                                                          | Restricted values | Default |
|------------------------------|------------------------------|-----------|------------------------------------|-------------------|--------|
| -s   HOSTNAME [HOSTNAME ...]  <!-- killServer -h [HOSTNAME ...] --> | --hostname HOSTNAME [HOSTNAME ...] <!-- killServer --hostname [HOSTNAME ...] --> | no        | Where the application is running  on more than one node, this identifies the node where you want to kill the server (so you can kill a server on a different node). Specify the Host Name, Host Names or "cluster" for all hosts | none | none |
| -f   <!-- killServer -f -->  | --force   <!-- killServer --force -->         | no        | forcefully kills a process (using kill -9)                                                                                                                                                                                         | none              | none |
|                              | --all     <!-- killServer --all -->           | no        | kills all processes, including   GENESIS_CLUSTER                                                                                                                                                                                     | none              | none |
| -c  <!-- killServer -c --> | --cluster   <!-- killServer --cluster -->       | no        | kills the server on all the nodes in the cluster                                                                                                                                                                                   | none              |    none | 

In the example below, there is a single node; this command kills all processes for the application, including GENESIS_CLUSTER.

 ```bash
killServer GENESIS_ROUTER --all
```

In the example below, there is a cluster; this command kills the nodes HARRY and GARY.

 ```bash
killServer GENESIS_ROUTER --hostname HARRY, GARY
```

## `LogLevel`

To change the logging levels by code (dynamically) on any Genesis process, use the `LogLevel` command.

###### Syntax
The `LogLevel` command can take the following arguments:
<!-- The LogLevel command has a list of possible arguments: -c, -DATADUMP_NACK_OFF, -DATADUMP_NACK_ON, -DATADUMP_OFF, -DATADUMP_ON,-h, -l, -p, -r, -STATUSDUMP_OFF, -STATUSDUMP_ON, -t -->

| Argument                               | Argument long name                   | Mandatory | Description                                 | Restricted values | Default |
|----------------------------------------|--------------------------------------|-----------|---------------------------------------------------------------------------------------|-------------------|-------|
| -c   <!-- LogLevel -c -->              |  --class `<class name>` <!-- LogLevel --class [CLASS NAME] -->   | no         | changes log level on the defined class       | none               | none |
|                                        | -DATADUMP_NACK_OFF  <!-- LogLevel -DATADUMP_NACK_OFF -->         | no         | changes log level to INFO for Genesis messages   | none   | none  |
|                                        | -DATADUMP_NACK_ON   <!-- LogLevel -DATADUMP_NACK_ON -->          | no         | changes log level to TRACE and captures only _NACK messages from Genesis messages     | none    | none  |
|                                        |  -DATADUMP_OFF   <!-- LogLevel -DATADUMP_OFF -->                 | no         | changes the log level to info for Genesis messages   | none | none  |
|                                        |  -DATADUMP_ON    <!-- LogLevel -DATADUMP_ON -->                  | no         | changes the log level to trace for Genesis messages   | none | none  |
| -h   <!-- LogLevel -h -->              |  --help  <!-- LogLevel --help -->                                | no         | show help on the command                     | none  | none  |
| -l  <!-- LogLevel -l -->               |  --level `<log level>`  <!-- LogLevel [LOG LEVEL] -->            | no         | log level - if log level is not correct, it will be set automatically to   DEBUG level | none     | none  |
| -p _process-name_,..,_process-name_ <!-- LogLevel -p --> |                                       | no         | attaches processes to the command     | none  | none  |
| -r _process-name_,..,_process-name_`<!-- LogLevel -r --> |                                       | no         | remove processes       | none        | none  |
|                                        | -STATUSDUMP_OFF   <!-- -STATUSDUMP_OFF -->              | no         | changes the log level to info for status updates    | none  | none  |
|                                        | -STATUSDUMP_ON    <!-- -STATUSDUMP_ON -->               | no         | changes the log level to trace for status updates  | none   | none  |
| -t `<time>`  <!-- LogLevel -t   -->    |                                                         | no         | duration of log level change in min/sec Eg: 1m, 1000s   | none | none  |

The example below sets the logging level for the GENESIS_AUTH_DATASERVER process to TRACE.

```bash
LogLevel -p GENESIS_AUTH_DATASERVER -l TRACE
```

The example below resets the LogLevel for the GENESIS_CLUSTER process.

```bash
LogLevel -r GENESIS_CLUSTER
```
The example below changes the log level to TRACE and captures only `_NACK` messages from Genesis messages for all processes.

```bash
LogLevel -DATADUMP_NACK_ON
```

## `MigrateAliases`

This migrates the Genesis alias store from database storage to file storage and vice versa. This is useful for debugging when you have FDB database technology.

###### Syntax
The `MigrateAliases` command can take the following arguments:
<!-- The MigrateAliases command has a list of possible arguments: -h, -o, -dst -->

| Argument | Argument long name                   | Mandatory | Description                                                     | Restricted values | Default |
|----------|--------------------------------------|-----------|-----------------------------------------------------------------|-------------------|---------|
| -h <!-- MigrateAliases -h -->  | --help <!-- MigrateAliases --help -->       | no	 | show help on the command | none | none |
| -o <!-- MigrateAliases -o -->  | --override <!-- MigrateAliases --override -->   | no	 | overwrite existing alias store if it exists in the destination | none | none |
| -dst <!-- MigrateAliases -dst --> | --destination <!-- MigrateAliases --destination --> | yes | destination dictionary store | DB or FILE | none |

The example below migrates the Genesis alias store to DB. It overwrites any existing dictionary in the destination.

```
MigrateAliases -dst=DB
```

### Database technology
FDB implementations use internal aliases for fields and tables. Migrating these aliases from database to a file will help to debug problems in the data storage.

- If you are running Genesis on a single node, use a file store.
- If you are running Genesis on more than one node, use database mode.

The `remap` operation updates the alias store, so if you are running a Genesis cluster it is better to use a database storage mode; this is less open to error, and you won't have to copy the alias storage file to the remaining nodes manually.

## `MigrateDictionary`

This migrates the Genesis dictionary from the Database Dictionary Store to the File Dictionary Store storage and vice versa.

The `MigrateAliases` command can take the following arguments:
<!-- The MigrateAliases command has a list of possible arguments: -h -o -dst -->

| Argument | Argument long name                   | Mandatory | Description                                                     | Restricted values | Default |
|----------|--------------------------------------|-----------|-----------------------------------------------------------------|-------------------|---------|
| -h <!-- MigrateAliases -h -->  | --help <!-- MigrateAliases --help -->       | no	 | show help on the command | none | none |
| -o <!-- MigrateAliases -o -->  | --override <!-- MigrateAliases --override -->   | no	 | overwrite existing alias store if it exists in the destination | none | none |
| -dst <!-- MigrateAliases -dst --> | --destination <!-- MigrateAliases --destination --> | yes | destination dictionary store | DB or FILE | none |

The example below migrates the Genesis dictionary from DB to FILE, it also overwrites any existing dictionary in the destination

```bash
 MigrateDictionary -dst=FILE -o
```

The script uses the [system definition file](/develop/server-capabilities/runtime-configuration/system-definition/#items-defined) to discover the `DictionarySource` property:

- If the property is `DB` (if the server uses a Database Dictionary Store), the `MigrateDictionary` script saves the dictionary to a file.
- If the `DictionarySource` is `FILE` (if the server uses a File Dictionary Store), the dictionary is saved to a database. The target database type - `DbLayer` - is also retrieved from the system definitions file.

Here is a recommendation:

- Use a file store (set by default) if you are running Genesis on a single node
- Use database store if you are running Genesis on more than one node

The `remap` operation updates the dictionary, so if you are running a Genesis cluster, it is better to use a Database Dictionary Store; this is more robust and you won't have to copy the dictionary file manually to the remaining nodes.

:::warning
It is potentially dangerous to switch the `DictionarySource` property.

If you run `remap` (which modifies the  dictionary) after `MigrateDictionary` and before switching the `DictionarySource` property, the file store and database store could contain different dictionaries and it is not safe to switch between them.
:::

## `mon`

This command shows the status of the overall system, so you can see if the server is up or not. By default, it displays a snapshot. If you want the screen to be refreshed, simply add a flag with the number of seconds between refreshes, such as `mon -10`.

```bash

***********************************************************************************************

Start: 2023-11-09 10:04:03                                                    Uptime: 3 minutes
Date:  2023-11-09 10:07:23                                Daemon status: OK. Node role: PRIMARY

PID     Process Name                  Port        Status         CPU       Memory    Message
===============================================================================================
547477  GENESIS_AUTH_CONSOLIDATOR     8005        STANDBY        27.50     2.80
547348  GENESIS_AUTH_DATASERVER       8002        HEALTHY        39.90     3.70      ;Lmdb currently uses 1% of available space (total size 2GB)
547308  GENESIS_AUTH_MANAGER          8001        HEALTHY        65.50     4.60
547390  GENESIS_AUTH_PERMS            8003        HEALTHY        35.00     3.50
547436  GENESIS_AUTH_REQUEST_SERVER   8004        HEALTHY        34.10     3.40
547520  GENESIS_CLUSTER               9000        HEALTHY        43.20     4.10      ;Lmdb currently uses 1% of available space (total size 2GB)
547567  GENESIS_ROUTER                9017        HEALTHY        33.20     3.50
===============================================================================================
n/a     ALPHA_CONSOLIDATOR            11002       MISSING        n/a       n/a       
547644  ALPHA_DATASERVER              11000       HEALTHY        34.20     3.90      ;Lmdb currently uses 1% of available space (total size 2GB)
547771  ALPHA_EVALUATOR               11003       STANDBY        25.50     2.80
547686  ALPHA_EVENT_HANDLER           11001       RUNNING        47.90     4.10

```
At the top, mon shows high-level system information:

| Information   | Description                                                                                                                                                                                                                                                                                                 |
|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Start         | Displays the last date and time the [`startServer`](/build-deploy-operate/operate/commands/#startserver) command was executed.                                                                                                                                                                              |
| Date          | Shows the current server date and time.                                                                                                                                                                                                                                                                     |
| Uptime        | Represents the total elapsed time since the [`startServer`](/build-deploy-operate/operate/commands/#startserver) was executed.                                                                                                                                                                              |
| Daemon status | The current status of the Genesis [Resource Daemon](/develop/development-environment/intellij-plugin/#resource-daemon). Possible values: MISSING, STARTING, OK.                                                                                                                                             |
| Node role     | The current node role in the Genesis Cluster, retrieved from the resource daemon [(see below)](#node-roles)  |

By default, the command shows the following columns of information per process:

| Column       | Description                                                                                                                                 |
|--------------|---------------------------------------------------------------------------------------------------------------------------------------------|
| PID          | The process ID, which is generated by the system                                                                                            |
| Process Name | The process name                                                                                                                            |
| Port         | The port where the process is operating; you can change the defaults in the **process.xml** file                                            |
| Status       | The status of the process [(see below)](#process-statuses)                                                                                  |
| CPU          | The percentage of CPU being used by the process. For multi-core CPUs or multiple CPUs, the total percentage of all processes can exceed 100 |
| Memory       | The percentage of RAM being used by the process                                                                                             |
| Message      | Displays any relevant message generated by the process                                                                                      |

### Node roles

When running in a [standard cluster](/build-deploy-operate/deploy/hosting-infrastructure/virtual-machines/#clustering), the possible values are:
* `PRIMARY` where this node is the primary node in the cluster
* `SECONDARY` where this node is a secondary node in the cluster
* `UNKNOWN` on initialization of a new node, or where there are connectivity issues to other nodes in the cluster

When running in a [Consul](/build-deploy-operate/deploy/hosting-infrastructure/containers/#clustering) cluster, the value is set to:
* `UNKNOWN` on initialization of a new node, or where there are connectivity issues to the Consul cluster
* `Consul` where the node is running successfully in a Consul cluster

### Process statuses
The key column in the display is **Status**, which can have the following values:

* `RUNNING` the process has just started and is still initializing.
* `STARTING` the process has completed initialization; it is running but hasn't yet signalled to the monitor that it has completed its internal health checks
* `HEALTHY` the process is fully available
* `STANDBY` when operating in a **regular cluster**, this indicates that the process is in standby mode; it will take over if the node becomes Primary; when operating in a **Consul cluster**, this indicates that the process is elected the primary singleton process
* `MISSING` there has been an error and the process has not started correctly

:::info
If you are using a development environment and you are running a local build via the IntelliJ plugin, you cannot run the `mon` command. The plugin provides a **Mon** tab where you can monitor the status of each process and start and stop each one.
:::

###### Syntax
The `mon` command can take the following arguments:
<!-- The mon command has a list of possible arguments: -h, -v, -c, -a, -m, -u, --down, --warning, --error, --unknown, --missing, --running, --starting, --standby, --healthy  -->

| Argument | Argument long name | Mandatory | Description                                                               | Restricted values | Default |
|----------|--------------------|-----------|---------------------------------------------------------------------------|-------------------|---------|
| -h  <!-- mon -h -->     | --help  <!-- mon --help -->           | no        | show help on the mon commands                                             | none              | none    |
| -v  <!-- mon -v -->     | --version  <!-- mon --version -->        | no        | show installed product versions                                           | none              | none    |
| -c  <!-- mon -c -->     | --cfg <!-- mon --cfg -->             | no        | show the config files used by each process                                | none              | none    |
| -a <!-- mon -a -->      | --all <!-- mon --all -->             | no        | show all information                                                      | none              | none    |
| -m  <!-- mon -m -->     | --monitors  <!-- mon --monitors -->       | no        | show state monitors  (the **Message** column heading becomes **Monitor**) | none              | none    |
| -u  <!-- mon -u -->     | --unhealthy <!-- mon --unhealthy -->       | no        | show unhealthy processes only                                             | none              | none    |
|          | --down  <!-- mon --down -->           | no        | show DOWN processes                                                       | none              | none    |
|          | --warning  <!-- mon --warning -->        | no        | show WARNING processes                                                    | none              | none    |
|          | --error   <!-- mon --error -->         | no        | show ERROR processes                                                      | none              | none    |
|          | --unknown  <!-- mon --unknown -->        | no        | show UNKNOWN processes                                                    | none              | none    |
|          | --missing  <!-- mon --missing -->        | no        | show MISSING processes                                                    | none              | none    |
|          | --running  <!-- mon --running -->        | no        | show RUNNING processes                                                    | none              | none    |
|          | --starting  <!-- mon --starting -->       | no        | show STARTING processes                                                   | none              | none    |
|          | --standby  <!-- mon --standby -->        | no        | show STANDBY processes                                                    | none              | none    |
|          | --healthy  <!-- mon --healthy -->        | no        | show HEALTHY processes                                                    | none              | none    |

:::info

Unhealthy processes includes all processes that are not `HEALTHY` or `STANDBY`.
`--unhealthy` can **not** be used with other status-filtering arguments.

:::

The example below runs `mon` with a polling interval of one second.

```bash
mon 5
```

The example below runs `mon`; it shows the status of every process and outputs the config files used by each process.

```bash
mon --cfg
```

The example below runs `mon`, but only shows processes that are unhealthy.

```bash
mon --unhealthy
```

The example below runs `mon`, but only shows processes that are missing.

```bash
mon --missing
```

## `PopulateHolidays`
This command populates the Holidays table with holidays, based on a specific year(s), country(ies) and region(s).

###### Syntax
The `PopulateHolidays` command can take the following arguments:
<!-- The PopulateHolidays command has a list of possible arguments: -c, -h, -r, -y -->

| Argument | Argument long name | Mandatory |               Description               | Restricted values |
|----------|--------------------|-----------|-----------------------------------------|-------------------|
| -c  <!-- PopulateHolidays -c -->     | --country (arg) <!-- PopulateHolidays --country (args) -->  | No        | the country name to search for holidays | No                |
| -h  <!-- PopulateHolidays -h -->     | --help <!-- PopulateHolidays --help -->            | No        | show help on this command               | No                |
| -r  <!-- PopulateHolidays -r -->     | --region (arg) <!-- PopulateHolidays --region (args) -->  | No        | the region name to search for holidays  | No                |
| -y <!-- PopulateHolidays -y -->      | --year (arg) <!-- PopulateHolidays --year (args) -->    | No        | the year of holidays                    | No                |

For example:

```bash
PopulateHolidays -y 2020,2021 -c BR,GB -r rj,en
```

## `PurgeTables`

This enables you to define the data-retention policy. Data will be removed from the database where the defined criteria are fulfilled.

Example: For the TRADE table, you could decide to keep allocated trades for 60 days, and 30 days for the rest.

To use this tool, you must have a `*-purger.kts` file in the application's `cfg` directory. Invoking the **PurgeTables** command will pick up all files that are found in the `$GENESIS_HOME/generated/cfg/` directory.

In order to enable syntax highlighting and autocompletion for purger files, you must add **genesis-environment** as a dependency of your application's **-config** module. See simple examples below for purger definitions:

A log file called `purge_<time_of_run>` will be created in the `$GENESIS_HOME/runtime/logs/` directory.

The functions and filters below give you different ways of purging data.

### Purge by date

The purger supports purging based on days. You can specify the max age of a record in terms of either calendar days or business days.

- Business days disregard weekends and public holidays.
- Calendar days disregard weekends by default, but can be set to include them, as shown in the example below.

To use this script, you need to supply
- a LONG or DATETIME field in the table to be used, or you can supply just the table name, in which case the TIMESTAMP field of the table will be used to calculate the age of the record.
- The max age of the record, which indicates all the records older than these days will be purged
- Optional fields when you use calendar days: boolean flag `ignoreWeekends` (defaults to true).
- Optional fields when you use business days: `country` and `region` name.

```kotlin
purgers {
 // purge trades over 180 days old based on field TRADE_DATE, not taking weekends into account (180 week-days)
 daysPurger(TRADE.TRADE_DATE, 180)
 // purge trades over 6 months old based on field TIMESTAMP of table which will be used internally, including weekends (180 full calendar days)
 daysPurger(TRADE, 180, ignoreWeekends = false)

 // purge prices older than 5 business days based on PRICE_DATETIME
 businessDaysPurger(PRICE.PRICE_DATETIME, 5)
 // purge prices older than 5 business days based on PRICE_DATETIME in country SPAIN
 businessDaysPurger(PRICE.PRICE_DATETIME, 5, "SPAIN")
}
```

### Purge by range
Range purger is similar to bulkPurger, but it performs efficient index range searches in database and speed up purger performance compared to bulkPurger.
- You need to give index name of the table you want to purge data on
- Purger has special function called `whereRange` which is used to filter based on index value

```kotlin
import global.genesis.gen.dao.repository.UserSessionRx3Repository
purgers {
 // Inject repo
 val userSessionRepo = inject<UserSessionRx3Repository>()

 config {
     batchSize = 1000
 }

 // Range purger with whereRange
 rangePurger(USER_SESSION.BY_USER_NAME).whereRange("JohnDoe")

 // Combination of filters for maximum flexibility
 rangePurger(USER_SESSION.BY_USER_NAME)
  .whereRange("JohnDoe")
  .filterBusinessDays(maxDays = 5, country = "SPAIN", region = "NATIONAL", field = USER_SESSION.LAST_ACCESS_TIME)

 rangePurger(USER_SESSION.BY_USER_NAME)
  .whereRange("JohnDoe")
  .filterDays(maxDays = 5)

 // finally clause usage
 rangePurger(USER_SESSION.BY_USER_NAME)
  .whereRange("JohnDoe")
  .finally { userSession ->
    println("Purged record: ${userSession.toGenesisSet()}")
  }
 // batchFinally clause usage
 rangePurger(USER_SESSION.BY_USER_NAME)
  .whereRange("JohnDoe")
  .batchFinally { userSessions ->
    println("Purged record: ${userSessions}")
  }
}
```

### Purge bulk data

You can purge data from the whole table using:

```kotlin
bulkPurger(USER_SESSION)
```
You can purge data from the table based on some conditions:

```kotlin
purgers {
// Purger that reads the whole user session every time it runs and deletes all sessions for JohnDoe
 bulkPurger(USER_SESSION).filter { it.userName == "JohnDoe" }
// Purge USER_SESSION older than 8 business days in country India
 bulkPurger(USER_SESSION)
  .filterBusinessDays(8, "INDIA")
}
```

### Custom purgers

If you need more complicated purging logic, you can create custom purgers.
These enable you to return a `Collection` or `Flow` of entities as a purger instruction.

For example, this custom purger removes all users who do not have a corresponding user session:

```kotlin
customPurger(USER) {
    val usersWithSession = entityDb.getBulk(USER_SESSION)
        .map { it.userName }
        .toSet()

    entityDb.getBulk(USER)
        .filterNot { it.userName in usersWithSession }
        .map { it.byName() }
        .asPurgerInstruction()
}
```

### Filters

The example above uses filters. This section looks at the filters that are available for purging. 

`whereRange`: This filter can contain the value of the table-index parameter or parameters if there is more than one field in the table-index. Parameter list can range from 1-10.
It filters the records based on the index parameter value or values. 

`filter`: filter based on the predicate provided

`filterBusinessDays`: this method enables you to purge data based on business date.
You need to provide:
- max age of record
- country name
- region (optional - defaults to "NATIONAL")
- LONG or DATETIME field of the table you want to purge (optional - defaults to TIMESTAMP field of table)

`filterDays`: this method enables you to purge data based on calendar date.
You need to provide:
- max age of record
- LONG or DATETIME field of the table you want to purge (optional - defaults to TIMESTAMP field of table)

### Config

You can also supply `config` values to control the behaviour of the purge. For example:

```kotlin
purgers {
    config {
        backupMode = CSV_GZ // accepted values: LOG, CSV, CSV_GZ, NONE
        formatMode = FORMATTED // accepted values: FORMATTED, LEGACY
        backupLocation = Paths.get(GenesisPaths.runtime(), "purger", OSSafeDateTimeString())
        batchSize = 1000
        maxParallelism = 16
    }

    // ...snip...
}
```

The example above uses `config` settings:

- `backupMode` specifies the type of log file that is created.
- `formatMode` corresponds to the DumpIt argument; records can be written in a [DumpIt](#dumpit) compatible `.csv` or `.csv.gz` file.
- `backupLocation` is where the the log file is located.
- `batchSize` represents the batch size of each purge (which results in a single database operation for each batch). Only use this when you are using `batchFinally` to include logic for [cascade purges](#cascade-purges). Larger batch sizes result in fewer database operations and a more efficient purge, but they do require more memory.
- `maxParallelism` is the max number of database threads that will be used concurrently in `purgeAllDerivedRanges` and `purgeAllDerivedRangesFlow` operations by default. If not specified, it will default to 16.

<br></br>For example, if you use `batchFinally` with `batchSize` set to 1,000 and you delete 8,000 records, there will be 8 database operations. By contrast, an unbatched operation using the `finally` filter would have a database operation for every single delete.

### Cascade purges

It is often necessary to cascade purges so that related records are also deleted. For example, if you are purging records from the TRADES table, you might also want to purge the related records in the FEES table.

For this, you can use either `finally` (which performs a delete for each record to be deleted) or `batchFinally` (which batches records and performs a single delete operation on the whole batch).

When you use `finally` or `batchFinally`, you can add extra functionality for every record that is purged, such as the logic for purging related records:

Basic example:

```kotlin
bulkPurger(TABLE_A).batchFinally { records ->
    val keysToPurge = records.map { TableB.byTableAId(it.tableId) }
    entityDb.purgeAllDerivedRecords(keysToPurge)
}

// or

bulkPurger(TABLE_A).finally { record ->
    entityDb.purgeDerivedRecord(TableB.byTableAId(record.tableId))
}
```

#### Available Cascade Methods

The following methods are available for cascading purges within your `finally` or `batchFinally` blocks:

| Method | When to Use | Return Value |
|--------|-------------|-------------|
| `purgeDerivedRecord(index)` | Delete a single related record | The deleted record, or `null` if not found (`E?`) |
| `purgeAllDerivedRecords(indices)` | Delete multiple related records (one per index) | List of all deleted records (`List<E>`) |
| `purgeDerivedRange(index)` | Delete all records matching a range (multiple records possible) | Nothing (`Unit`) |
| `purgeAllDerivedRanges(indices, maxParallelism)` | Delete all records matching multiple ranges | Nothing (`Unit`) |
| `purgeDerivedRangeFlow(index)` | Process records as they're being purged from a range | A flow of deleted records (`Flow<E>`) |
| `purgeAllDerivedRangesFlow(indices, maxParallelism)` | Process records as they're being purged from multiple ranges | A flow of deleted records (`Flow<E>`) |
| `purgeAllDerivedRecordsFlow(indices)` | Process records as they're being purged from unique indices | A flow of deleted records (`Flow<E>`) |

**Notes:**
- `indices` is a list of index objects (e.g., `Fee.byTradeId(tradeId)`).
- `maxParallelism` is optional and controls how many ranges are processed in parallel.
- `Flow<E>` is a Kotlin flow, which allows you to process each deleted record as it is purged.
- Non-Flow methods return the deleted records directly (as a single record, a list, or nothing for range deletes).
- Flow methods are useful for streaming or real-time processing of deleted records.

#### Real-World Examples

Deleting User Sessions and Related Permissions

```kotlin
bulkPurger(USER_SESSION).batchFinally { sessions ->
    // Delete related permissions (one per user)
    val permissionIndices = sessions.map { RightSummary.byUserNameRightCode(it.userName, "meh") }
    val deletedPermissions = entityDb.purgeAllDerivedRecords(permissionIndices)
    println("Deleted ${deletedPermissions.size} permission records")
}
```

Deleting Trades and All Related Fees with Logging

```kotlin
bulkPurger(TRADE).batchFinally { trades ->
    // Delete all fees for each trade with logging
    val feeIndices = trades.map { Fee.byTradeId(it.tradeId) }
    entityDb.purgeAllDerivedRangesFlow(feeIndices)
        .collect { fee ->
            println("Deleting fee ${fee.feeId} for trade ${fee.tradeId}")
        }
}
```

Single Record Cascade with Return Value

```kotlin
bulkPurger(TRADE).finally { trade ->
    // Delete the single fee record for this trade
    val deletedFee = entityDb.purgeDerivedRecord(Fee.byTradeId(trade.tradeId))
    if (deletedFee != null) {
        println("Deleted fee ${deletedFee.feeId} for trade ${trade.tradeId}")
    }
}
```

Processing Records During Purge

```kotlin
bulkPurger(USER_SESSION).batchFinally { sessions ->
    val rightIndices = sessions.map { RightSummary.byUserNameRightCode(it.userName) }
    entityDb.purgeAllDerivedRecordsFlow(rightIndices)
        .collect { right ->
            // Process each right as it's being deleted
            println("Removing right ${right.rightCode} for user ${right.userName}")
        }
}
```

#### Important considerations

All cascade purge methods automatically:
- Create backups of records before deletion (based on your `backupMode` configuration)
- Handle errors gracefully and log failures
- Continue processing even if individual cascade operations fail
- The purger configuration settings such as batch size and backup location will be respected.

Also:
- Use `batchFinally` with `purgeAllDerivedRecords` or `purgeAllDerivedRanges` for better performance when processing many records
- Use `finally` with `purgeDerivedRecord` or `purgeDerivedRange` for simpler cases with individual records

## `ReconcileDatabaseSync`

<!-- The ReconcileDatabaseSync command has a list of possible arguments: -d, -f, -H, -P, -u, -p, -s, -n, -i, -h -->

This is used to check if there are differences between a local DB and a remote DB with common dictionary tables.

Typically, this would be used to reconcile tables that are being kept in sync by the GENESIS_SYNC process.

The local DB's details (host, port, user, etc) are read from the system definition file in the local environment. The remote DB's details are specified as options to the command.

The tables to check are specified in the **genesis-sync-definition.xml** file. Here is a simple example that specifies two tables:

```xml
<sync>
    <tables>
        <table name="TRADE"/>
        <table name="ORDER"/>
    </tables>
</sync>
```

If there are any differences found between the local and remote tables, then the result will be output to a text file. The location of this text file will be displayed on screen.
There are 3 categories in the output: "Records Missing From Local", "Records Missing From Remote" and "Records Which Differ Between Remote and Local".

Records are compared using the primary key field. The TIMESTAMP and RECORD_ID fields are extremely likely to differ,
because the tables being compared are in separate databases. Therefore, these fields are not compared.

###### Arguments

| Argument | Argument long name | Mandatory | Description | Restricted Values |
| -- | -- | -- | -- | -- |
| -d <!-- ReconcileDatabaseSync -d --> | --dblayer <!-- ReconcileDatabaseSync --dblayer --> | true | Database Layer type | Yes: FDB, FDB2, SQL, SIMPLE |
| -f <!-- ReconcileDatabaseSync -f --> | --fdb <!-- ReconcileDatabaseSync --fdb --> | false | FDB cluster file name |No |
| -H <!-- ReconcileDatabaseSync -H --> | --host <!-- ReconcileDatabaseSync --host --> | false | Remote DB hostname |No |
| -P <!-- ReconcileDatabaseSync -P --> | --port <!-- ReconcileDatabaseSync --port --> | false | Remote DB port |Yes: Number > 0 |
| -u <!-- ReconcileDatabaseSync -u --> | --username <!-- ReconcileDatabaseSync --username --> | false | DB user username |No |
| -p <!-- ReconcileDatabaseSync -p --> | --password <!-- ReconcileDatabaseSync --password --> | false | DB user password |No |
| -s <!-- ReconcileDatabaseSync -s --> | --nullstring <!-- ReconcileDatabaseSync --nullstring --> | false | Evaluate null and empty strings as equal |No value required |
| -n <!-- ReconcileDatabaseSync -n --> | --numdays <!-- ReconcileDatabaseSync --numdays --> | false | Only compare records with timestamps between now and the number of days specified. If either DB has matching records outside this timestamp range, this will not be flagged as a reconciliation difference |Yes: Number > 0 |
| -i <!-- ReconcileDatabaseSync -i --> | --ignorefields <!-- ReconcileDatabaseSync --ignorefields --> | false | Comma-separated list of additional fields to ignore ("RECORD_ID" and "TIMESTAMP" are always ignored) |No |
| -h <!-- ReconcileDatabaseSync -h --> | --help <!-- ReconcileDatabaseSync --help --> | false | Show usage information |No |

###### Examples

This is a simple run with a remote Postgres DB:

```bash
ReconcileDatabaseSync -d SQL -H "jdbc:postgresql://dbhost:5432/" -u dbuser -p dbpass
```

This example runs with a remote Postgres DB. it evaluates null and empty strings as equal; it compares records up to 2 days ago, and it ignores the field STATUS.

```bash
ReconcileDatabaseSync -d SQL -H "jdbc:postgresql://dbhost:5432/" -u dbuser -p dbpass -s -n 2 -i STATUS
```


## `remap`

Remap is a schema-migration tool used to apply the current schema (defined in the deployed field and table GPAL dictionaries) to the underlying database layer used by the Genesis Application Platform.

The `remap` command performs the following tasks:

- It reads all dictionary files (fields.kts, tables.kts and view.kts) from **$GC** and compares these to the previously generated schema. It uses these changes to remap the memory-resident database.
- It generates dao objects based on the dictionary tables, so you can perform database operations in a type-safe way.
- If you are running FDB, it updates the Genesis alias store.

If you run `remap` with no arguments, it simply gives a report of changes that exist in the configuration.

If you want to commit the changes to the database, you must use the **--commit** argument:

```bash
remap [-c | --commit]
```
For full details, see our page on the [remap](/build-deploy-operate/operate/commands/remap/) command. 

## `RenameFields`
This command is used to rename a field name in a database without changing the dictionary or config files.

###### Syntax
The `RenameFields` command takes two arguments; both of which are mandatory:

```bash
RenameFields [-i <[current name of field]>] [-o  <[new name of field]>]
```
<!-- The RenameFields command has a list of possible arguments: -i, -o, -h -->

| Argument | Argument long name | Mandatory | Description                              | Restricted values | Default |
|----------|--------------------|-----------|------------------------------------------|-------------------|---------|
| -i <!-- RenameFields -i -->      | --input <!-- RenameFields --input -->           | yes       | name of field that you want to change    | none              | none    |
| -o <!-- RenameFields -o -->      | --output <!-- RenameFields --output -->          | yes       | name you want the field to be changed to | none              | none    |
| -h <!-- RenameFields -h -->      | --help  <!-- RenameFields --help -->           | no        | show help on using this command          | none              | none    |


The `--input` argument represents the name of the field you would like to change. The argument must be an existing field name in the database.

The `--output` argument represents the name of the field you would like to change to. The argument must also be an existing field name in the database.

Both arguments must also be of the same type.

If both arguments are in the same table, it would result in the `--output` field being deleted.

All changes using `RenameFields` can be changed back to the original database schema by using the command `remap --commit`.

For example:

```bash
RenameFields -i SYMBOL -o TRADE_ID
```

This changes the name of the SYMBOL field to TRADE_ID.

Another example:

```bash
RenameFields --input FIRST_NAME --output FNAME
```

This changes the name of the field FIRST_NAME to FNAME.

Invalid example:

```bash
RenameFields -i PRICE -o FIRST_NAME
```

This would result in an error, as PRICE is of type DOUBLE while FIRST_NAME is of type STRING.

## `SendIt`

To send data into the database (inserts, modifies and upserts), use the `SendIt` command. 

The data you want to send must be in a .csv file or files:

- The file must be in the directory that you are running the command from. 
- The name of the file (or files) should match the name of the table where the new or amended data is to be sent. **If you want to use a different filename, you must specify this using the `-f` argument.**

:::info
Make sure your data is in the correct format before sending to your target table. 
For example:
- DATE values should be in the format of `yyyy-MM-dd` (for example 2026-05-25).
- DATETIME values should be in the format of `yyyy-MM-dd HH:mm:ss.SSS Z` (for example 2026-05-25 12:30:01.123 +0000).

If you're having trouble sending in data, use the verbose option with `-v` to get a row-by-row report!
:::

###### Syntax
The `SendIt` command can take the following arguments:
<!-- The SendIt command has a list of possible arguments: -a, -d, -cf, -f, -fm, -h, -m, -mf, -quiet, -r, -t, -u, -v -->

| Argument | Argument long name     | Mandatory | Description                                                    | Restricted values     | Default |
|----------|------------------------|-----------|----------------------------------------------------------------|-----------------------|--------|
| -a <!-- SendIt -a -->      | --all  <!-- SendIt --all -->                | no        | import all the tables from all the csv files to the database | no                    | none    |
| -d <!-- SendIt -d -->      | --delete <!-- SendIt --delete  -->              | no        | perform delete operations on all records                     | no                    | none    |
| -cf <!-- SendIt -cf -->     | --columnFormat <!-- SendIt --columnFormat -->        | no        | set specific date format for column, for example `-cf DATE=yyyyMMdd` | no                    | none    |
| -f  <!-- SendIt -f -->     | --file (arg) <!-- SendIt --file (args) -->        | no        | name of the csv file that contains the data                  | no                    | Genesis looks for a new .csv file whose name matches the name of the source table    |
| -fm <!-- SendIt -fm -->     | --formatMode (arg) <!-- SendIt --formatMode (args) -->  | no        | FORMATTED takes field formats into account; LEGACY does not  | FORMATTED and LEGACY  | LEGACY  |
| -h  <!-- SendIt -h -->     | --help <!-- SendIt --help -->                | no        | show help on how to use this command                         | no                    | none    |
| -m  <!-- SendIt -m -->     | --modify (arg) <!-- SendIt --modify (args) -->      | no        | key name used to find original record                        | no                    | none    |
| -mf <!-- SendIt -mf -->     | --modifyFields (arg) <!-- SendIt --modifyFields (args) --> | no        | specifies fields to modify (only used with `-m`)             | no                    | none    |
| -quiet <!-- SendIt -quiet -->   | --quietMode  <!-- SendIt --quietMode -->          | no        | make database changes without triggering real-time updates in update queue layer | no                    | none    |
| -r <!-- SendIt -r -->      | --recover <!-- SendIt --recover -->             | no        | perform recover operations on all records; this is a special operation meant to preserve the original timestamps; **use with caution**. Only use this when you want to restore a system after completely erasing the database tables. You must use only untouched files from a real back-up of the original dataset. There are no other circumstances in which you should use this option. Ever | no                    | none    |
| -t <!-- SendIt -t -->      | --table (arg) <!-- SendIt --table (args) -->        | yes       | the name of the database table to be updated or amended      | must be a valid table | none    |
| -u  <!-- SendIt -u -->     | --upsert (arg) <!-- SendIt --upsert (args) -->      | no        | upserts records from a single csv file       | no                    | none    |
| -ua  <!-- SendIt -ua -->   | --upsertAll (arg) <!-- SendIt --upsertAll (args) --> | no        | upserts records from multiple csv files      | no                    | none    |
| -v  <!-- SendIt -v -->     | --verbose <!-- SendIt --verbose -->             | no        | log every error line to output                               | no                    | none    |

For example:

```bash
SendIt -t FUND -f FUND.csv
```

This reads the **FUND.csv** file in the local directory and inserts the data from the file into the FUND table.

### Modifying and upserting

:::info
You can use SendIt to make changes to the database while the application is running; Data Servers and Consolidators will process any changes in the normal way (for example). But if you are in any doubt that your changes might clash with other changes happening in the application, you should switch off all processes before using SendIt.
:::

To **modify** records, create a .csv file of the records that need to be modified, including the key value and the modified values for each record. Then use SendIt with the `-m` flag to specify the key that identifies the record or records to be modified. **You cannot modify the key that you supply here - although you can modify other key fields (with care).** For example, you can't change an ID in the file and then modify on_BY_ID key.

The following example:

- looks for a file called **FUND.csv** by default.
- compares each record in the FUND table with the record that has the same ID in the csv file
- applies any changes in the csv record to the record in the table

```bash
SendIt -t FUND -m FUND_BY_ID
```

Modify fields (`-mf`) is an extra parameter that can be added to `-m` operations. SendIt only attempts to modify the record fields specified in this comma-separated list. For example:

```bash
SendIt -t ALL_TRADES -m TRADE_BY_TRADE_ID -mf TRADE_PRICE, TRADE_QUANTITY
```

To **upsert** records, create a .csv file of the records that need to be modified or inserted, including the key value and the relevant values for each record. Then use SendIt with the `-u` flag to specify the key that identifies the original record from each row in the csv file.

The following example:

- looks for a file called **FUND.csv** by default
- for each record in the FUND table that has a matching ID in the csv, it applies the csv changes to the table
- for any record in the csv file that has no matching ID in the FUND table, the record is inserted into the table

```bash
SendIt -t FUND -u FUND_BY_ID
```
### Deleting
To **delete** records, specify `-d` (or `--delete`)

```bash
SendIt -t FUND -d
```
### Verbose mode

Verbose mode additionally outputs line-by-line operation outcome, and a final summary of error lines to be corrected and resubmitted. This makes `SendIt` useful for scheduled or automated jobs (e.g. daily data loads).

:::warning
Do not use `SendIt` to update User details in any way. This can easily cause database errors. To update User profiles or User attributes, only use Genesis [user entity management](/develop/client-capabilities/grids/entity-manager/).
:::

## `SetAutoIncrement`

:::warning
Stop all your application's processes before using this command.
:::

This command enables you to set the next number to be generated for one or more auto-incrementing fields.

You can set the value for a single field using the `-v` argument.

You can set values for more than one field by supplying the details in a csv file and using the `-f` argument. The file should take the following format:

```
table, field, value
TRADE, TRADE_ID, 1
ORDER, ORDER_ID, 10538
```

By default, this command expects the file to be called **AutoIncrementValues.csv**, but you can specify a different file name.

###### Syntax
The `SetAutoIncrement` command can take the following arguments:
<!-- The SetAutoIncrement command has a list of possible arguments: -f, -h, -s, -t, -v -->

| Argument | Argument long name | Mandatory | Description                                                                                           | Restricted values | Default |
|----------|--------------------|-----------|-------------------------------------------------------------------------------------------------------|-------------------|-------|      
| -f <!-- SetAutoIncrement -f -->      | --file (arg) <!-- SetAutoIncrement --file (args) -->    | no | name of the csv file containing the new sequence numbers (this overrides any --value option supplied) | none | AutoIncrementValues.csv |
| -h <!-- SetAutoIncrement -h -->      | --help <!-- SetAutoIncrement --help -->            | no | displays help on using this command                                                                   | none | none | 
| -s <!-- SetAutoIncrement -s -->      | --field (arg) <!-- SetAutoIncrement --field (args) -->   | no | name of the auto-increment field (when setting the next sequence number for a single field)           | none | none |
| -t <!-- SetAutoIncrement -t -->      | --table (arg) <!-- SetAutoIncrement --table (args) -->   | no | name of the table that has the auto-increment (when setting the next sequence number for a single field) | none | none |
| -v <!-- SetAutoIncrement -v -->      | --value (arg)  <!-- SetAutoIncrement --value (args) -->  | no | new sequence number (integer) for the auto-increment (when setting the auto-increment for a single field) | none | none |

Note that all fields are optional in principle. But when setting a single field value, you must provide `-t` `-s` and `-v`.

The example below sets the number 101 as the next number to be generated for the TRADE_ID field in the TRADE table.

```bash
SetAutoIncrement -t TRADE -s TRADE_ID -v 101
```

The example below uses the file **autoIncVals** to set auto-increment values for multiple fields.

```bash
SetAutoIncrement -f autoIncVals.csv
```

## `SetSequence`

:::warning
Stop all your application's processes before using this command.
:::

This command enables you to set the next sequence number for one or more sequence fields.

- To set the value for a single sequence, use the `-v` argument.

- To set values for more than one sequence, supply the details in a csv file (for example, a file that you have exported using either `GetNextSequenceNumbers` or `GetSequenceCount`) and use the `-f` argument. The file should take the following format:

```
"Table","Sequence","Value"
"USER","US","1303"
"PROFILE","PR","0"
"RIGHT","RI","0"
```

`SetSequence` must only be run when the system processes have been stopped. After running `SetSequence`, you need to [restart the server](/build-deploy-operate/operate/commands/#startserver).

###### Syntax
The `SetSequence` command can take the following arguments:
<!-- The SetSequence command has a list of possible arguments: -f, -h, -s, -t, -v -->

| Argument | Argument long name | Mandatory | Description                                                                                                        | Restricted values | Default|
|----------|--------------------|-----------|--------------------------------------------------------------------------------------------------------------------|---------------|-------|       
| -f <!-- SetSequence -f -->      | --file (arg) <!-- SetSequence --file (args) -->    | no        | name of csv file containing table-sequence-value sets (these override any --sequence and --value options supplied) | none | none |
| -h <!-- SetSequence -h -->      | --help <!-- SetSequence --help -->            | no        | displays help on how to use the command                                                                            | none | none |
| -s  <!-- SetSequence -s -->     | --sequence (arg) <!-- SetSequence --sequence (args) --> | no        | two-character ID for the sequence (if setting an individual value)                                                 | none | none |
| -t <!-- SetSequence -t -->      | --table (arg) <!-- SetSequence --table (args) -->   | no        | name of the table that contains the sequence (if setting an individual value)                        | none | none |
| -v  <!-- SetSequence -v -->     | --value (arg) <!-- SetSequence --value (args) -->   | no        | new integer value to be set (if setting an individual value)                                                       | none | none |

All fields are optional in principle. But if you are setting the value for a single sequence, you must provide a `-t` `-s` and `-v`.

The example below sets the sequence AA in the table TRADE for the TRADE_ID field; the next sequence number to be generated is set to 1001.

```
SetSequence-t TRADE -s AA -v 1001
```
The example below uses the file **SeqVals** to set the next sequence number for multiple sequences.

```bash
SetSequence -f SeqVals.csv
```

## `startProcess`

This command starts a Genesis process defined in the [processes.xml](/develop/server-capabilities/runtime-configuration/processes/) file.

###### Syntax

The command must be followed by the name of the process that you want to start.

The following arguments are also available:
<!-- The startProcess command has a list of possible arguments: -s, -v, --dump -->

| Argument                   | Argument long name                          | Mandatory | Description                                                                                                                                                                                         | Restricted values | Default |
|----------------------------|---------------------------------------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-------|
| -s HOSTNAME [HOSTNAME ...] <!-- startProcess -s [HOSTNAME ...] -->  | --hostname HOSTNAME [HOSTNAME ...] <!-- startProcess --hostname [HOSTNAME ...]  --> | no        | where the application is running on more than one node, this identifies the node where you want to start the process (so you can start a process on a different node). Specify the Host Name | none                |	none   |  
| -v    <!-- startProcess -v  -->                                     | --verbose   <!-- startProcess --verbose  -->                                | no        | starts in verbose mode; logs to the terminal so you can view on screen   | none                | none   |  
|                                                                     | --dump   <!-- startProcess --dump  -->                 | no        | displays progress of the process, which is useful for debugging                                                                                                                          | none                |		none   |  
   

The following example starts the process called APP_EVALUATOR:

```bash
startProcess APP_EVALUATOR
```

The following example starts the GENESIS_CLUSTER process and displays any useful debugging information in the console:

```bash
startProcess GENESIS_CLUSTER --dump
```

The following example starts the APP_NOTIFY process on node1 only:

```bash
startProcess APP_NOTIFY -s node1
```

:::info
The `-s` option requires the `EnableDaemonScriptController` system definition property to be enabled and set to "true". This will allow the daemon to execute commands on the specified node, as well as reach out to other existing daemons, as long as the network configuration allows it. This behaviour may be considered insecure, so it is disabled by default.
:::

The script looks in the [processes.xml](/develop/server-capabilities/runtime-configuration/processes/) file to find out how to start the process. For example `startProcess GENESIS_AUTH_DATASERVER` starts the process with the correct classpath and extra arguments defined in the `processes.xml` file. Something similar to:

```bash
java -Xmx256m -DXSD_VALIDATE=false global.genesis.process.GenesisProcessBootstrap -name GENESIS_AUTH_DATASERVER -scan global.genesis.dataserver.pal -module genesis-pal-dataserver -script genesis-dataserver.kts -loggingLevel INFO,DATADUMP_OFF >/dev/null 2> $L/GENESIS_AUTH_DATASERVER.log.err &
```

###### Logging
`startProcess` will redirect the standard error stream to a log file in the `$L` directory. The log file will be named `<process_name>.log.err` (for example, `GENESIS_AUTH_DATASERVER.log.err`).
If the `RedirectStreamsToLog` system definition is set to `true` (by default is `false`), this log file will not be created and the output of the standard error stream will be written to the standard process log file managed by Log4j2.

`startProcess` will also compress and move existing process logs in the `$L` directory by default (see [logging](/build-deploy-operate/operate/logging) for more information) to the `$RL` directory. This only affects the logs of the process you are starting.
If the `LogArchivingEnabled` system definition setting is set to `false` (by default is `true`), `startProcess` will not attempt to compress and move existing process logs. Please ensure the Log4j2 configuration is set up correctly to handle log rotation if this is the case.


## `startServer`

This command starts all the processes defined for an application. It reads the **$GC/processes.xml** file to determine which processes to start and how to start them.

###### Syntax

The `startServer` command can take the following arguments:
<!-- The startServer command has a list of possible arguments: -s, -c, -i, -v -->

| Argument                    | Argument long name                   | Mandatory | Description                                                                                                                                                                                | Restricted values | Default |
|-----------------------------|--------------------------------------|-----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|--------|
|  -s HOSTNAME [HOSTNAME ...] <!-- startServer -s [HOSTNAME ...] --> | --hostname   HOSTNAME [HOSTNAME ...] <!-- startServer --hostname [HOSTNAME ...] -->| no        | if the application is running on more than one node, this identifies the node where you want to start the server (so you can start a server on a different node). Specify the Host Name | none | none |
|  -c  <!-- startServer -c  -->                                      | --cluster   <!-- startServer --cluster  -->                                        | no        | starts the server on every node in the cluster | none |none |
|  -i  <!-- startServer -i  -->                                      | --ignoreDaemon  <!-- startServer --ignoreDaemon  -->                               | no        | avoids killing/starting the daemon | none | none |
| -v   <!-- startServer -v  -->                                      | --verbose   <!-- startServer --verbose  -->                                        | no        | starts in verbose mode; logs to the terminal so you can view on screen   | none   | none   |  

The example below starts the server.

```
startServer
```

The example below starts the server on node1 without starting/killing the daemon.

```
startServer  -s node1 -i
```

:::info
The -s option requires the `EnableDaemonScriptController` system definition property to be enabled and set to "true". This will allow the daemon to execute commands on the specified node, as well as reach out to other existing daemons, as long as the network configuration allows it. This behaviour may be considered insecure, so it is disabled by default.
:::

###### Logging
`startServer` will also compress and move existing process logs in the `$L` and `$RL` directories by default (see [logging](/build-deploy-operate/operate/logging) for more information) to the `$AL` directory. This affects all existing log files.
If the `LogArchivingEnabled` system definition setting is set to `false` (by default is `true`), `startServer` will not attempt to compress and move existing process logs. Please ensure the Log4j2 configuration is set up correctly to handle log rotation if this is the case.

